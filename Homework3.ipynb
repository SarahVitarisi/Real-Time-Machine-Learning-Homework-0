{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3d6e96fd",
      "metadata": {
        "id": "3d6e96fd"
      },
      "outputs": [],
      "source": [
        "import imageio \n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dd17c817",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd17c817",
        "outputId": "bb55dfd9-46cd-4cd3-f43e-ecbc4eee82f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "805741fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "a71fd439aa9940ddb1fdd20d64764059",
            "5635eba58f824c2288c47754f5de6b45",
            "e2f51453be7a44bd940f8b2eb2ef5d7b",
            "5c2e5d5f5b7e4e248c825d20e7f75c84",
            "8bb190912b114fa5858c4d5ec10a364c",
            "ecc8d0ee7bf4487d930487ca2899d5d0",
            "11579493c8244feabdbcce11555b8687",
            "26bca5a0e4b54cd4855bdaf6aef8b701",
            "9aeb455ecd134a29b93eb0651c5a9901",
            "20102e529da94e32bda8baaec05fcba0",
            "247119e71474422d8aac56e71a08ca34"
          ]
        },
        "id": "805741fc",
        "outputId": "1728c331-e9a7-485e-b752-9c05d0b95811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a71fd439aa9940ddb1fdd20d64764059"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c2109011",
      "metadata": {
        "id": "c2109011"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "            self.act1 = nn.Tanh()\n",
        "            self.pool1 = nn.MaxPool2d(2)\n",
        "            self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "            self.act2 = nn.Tanh()\n",
        "            self.pool2 = nn.MaxPool2d(2)\n",
        "            self.fc1 = nn.Linear(8*8*8, 32)\n",
        "            self.act3 = nn.Tanh()\n",
        "            self.fc2 = nn.Linear(32,10)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            out = self.pool1(self.act1(self.conv1(x)))\n",
        "            out = self.pool2(self.act2(self.conv2(out)))\n",
        "            out = out.view(-1, 8*8*8)\n",
        "            out = self.act3(self.fc1(out))\n",
        "            out = self.fc2(out)\n",
        "            return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e34c2675",
      "metadata": {
        "id": "e34c2675"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_train += loss.item()\n",
        "    print('{} Epoch {}, Training loss {}'.format(\n",
        "    datetime.datetime.now(), epoch,\n",
        "    loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7e0db57b",
      "metadata": {
        "id": "7e0db57b"
      },
      "outputs": [],
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                batchsize = imgs.shape[0]\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name, correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0486db04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0486db04",
        "outputId": "d265bc1d-31f1-41cb-e7d2-18a30ca6c05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 16:30:58.227099 Epoch 1, Training loss 2.042073919949934\n",
            "2022-03-30 16:31:14.352645 Epoch 2, Training loss 1.7447231538460384\n",
            "2022-03-30 16:31:30.294415 Epoch 3, Training loss 1.5877545703097682\n",
            "2022-03-30 16:31:46.194301 Epoch 4, Training loss 1.4978074417699634\n",
            "2022-03-30 16:32:02.224197 Epoch 5, Training loss 1.4370534296535775\n",
            "2022-03-30 16:32:18.189360 Epoch 6, Training loss 1.3837433657835208\n",
            "2022-03-30 16:32:34.196557 Epoch 7, Training loss 1.3324004486393746\n",
            "2022-03-30 16:32:50.137744 Epoch 8, Training loss 1.286688561482198\n",
            "2022-03-30 16:33:06.075392 Epoch 9, Training loss 1.2455877017639483\n",
            "2022-03-30 16:33:21.943076 Epoch 10, Training loss 1.2086571809428428\n",
            "2022-03-30 16:33:37.906456 Epoch 11, Training loss 1.1735460141583172\n",
            "2022-03-30 16:33:54.107778 Epoch 12, Training loss 1.1465314299706608\n",
            "2022-03-30 16:34:10.308463 Epoch 13, Training loss 1.120724389086599\n",
            "2022-03-30 16:34:26.406269 Epoch 14, Training loss 1.0982416886502824\n",
            "2022-03-30 16:34:42.483419 Epoch 15, Training loss 1.0783878862857819\n",
            "2022-03-30 16:34:58.458573 Epoch 16, Training loss 1.059521066105884\n",
            "2022-03-30 16:35:14.435156 Epoch 17, Training loss 1.0427480320186566\n",
            "2022-03-30 16:35:30.303125 Epoch 18, Training loss 1.0256883898354552\n",
            "2022-03-30 16:35:46.334886 Epoch 19, Training loss 1.0119840854878925\n",
            "2022-03-30 16:36:02.330863 Epoch 20, Training loss 0.99942938156445\n",
            "2022-03-30 16:36:18.279089 Epoch 21, Training loss 0.9849595733158424\n",
            "2022-03-30 16:36:34.563061 Epoch 22, Training loss 0.9746148397245675\n",
            "2022-03-30 16:36:52.856897 Epoch 23, Training loss 0.966356282953716\n",
            "2022-03-30 16:37:09.327468 Epoch 24, Training loss 0.9557260792426137\n",
            "2022-03-30 16:37:27.742600 Epoch 25, Training loss 0.9455116340114028\n",
            "2022-03-30 16:37:46.109901 Epoch 26, Training loss 0.9370941864255139\n",
            "2022-03-30 16:38:02.821888 Epoch 27, Training loss 0.9305671026639621\n",
            "2022-03-30 16:38:21.037189 Epoch 28, Training loss 0.9235343484165114\n",
            "2022-03-30 16:38:39.063130 Epoch 29, Training loss 0.9148776876499586\n",
            "2022-03-30 16:38:55.500813 Epoch 30, Training loss 0.9078859474957751\n",
            "2022-03-30 16:39:11.583291 Epoch 31, Training loss 0.9012645775704737\n",
            "2022-03-30 16:39:27.912329 Epoch 32, Training loss 0.8942392710834512\n",
            "2022-03-30 16:39:44.460466 Epoch 33, Training loss 0.8885958076590468\n",
            "2022-03-30 16:40:00.401905 Epoch 34, Training loss 0.8829197064232643\n",
            "2022-03-30 16:40:16.315848 Epoch 35, Training loss 0.8761261764847105\n",
            "2022-03-30 16:40:32.285775 Epoch 36, Training loss 0.8712200341779558\n",
            "2022-03-30 16:40:48.350622 Epoch 37, Training loss 0.8652157051788877\n",
            "2022-03-30 16:41:04.417131 Epoch 38, Training loss 0.8583310490374065\n",
            "2022-03-30 16:41:20.483508 Epoch 39, Training loss 0.8540690465808829\n",
            "2022-03-30 16:41:36.490976 Epoch 40, Training loss 0.8504384108973891\n",
            "2022-03-30 16:41:52.561445 Epoch 41, Training loss 0.8451158829662196\n",
            "2022-03-30 16:42:08.519292 Epoch 42, Training loss 0.8403920610542492\n",
            "2022-03-30 16:42:24.395591 Epoch 43, Training loss 0.8350308193727527\n",
            "2022-03-30 16:42:40.251802 Epoch 44, Training loss 0.8328142654331748\n",
            "2022-03-30 16:42:56.274605 Epoch 45, Training loss 0.8265898173574902\n",
            "2022-03-30 16:43:12.108325 Epoch 46, Training loss 0.823038871407204\n",
            "2022-03-30 16:43:27.979539 Epoch 47, Training loss 0.8181226974176934\n",
            "2022-03-30 16:43:43.947022 Epoch 48, Training loss 0.8162149216436669\n",
            "2022-03-30 16:43:59.892008 Epoch 49, Training loss 0.8112273798193164\n",
            "2022-03-30 16:44:16.023767 Epoch 50, Training loss 0.8065264747118401\n",
            "2022-03-30 16:44:32.029910 Epoch 51, Training loss 0.8037232078630906\n",
            "2022-03-30 16:44:48.106761 Epoch 52, Training loss 0.7999229762712707\n",
            "2022-03-30 16:45:04.206662 Epoch 53, Training loss 0.7964761143983783\n",
            "2022-03-30 16:45:20.190669 Epoch 54, Training loss 0.7926304612089606\n",
            "2022-03-30 16:45:36.200353 Epoch 55, Training loss 0.7905637212192921\n",
            "2022-03-30 16:45:52.246124 Epoch 56, Training loss 0.7875492635666562\n",
            "2022-03-30 16:46:08.151058 Epoch 57, Training loss 0.7841157454358952\n",
            "2022-03-30 16:46:24.074870 Epoch 58, Training loss 0.7805175783155519\n",
            "2022-03-30 16:46:40.105074 Epoch 59, Training loss 0.7778122750160944\n",
            "2022-03-30 16:46:56.014142 Epoch 60, Training loss 0.7745436786309533\n",
            "2022-03-30 16:47:11.891180 Epoch 61, Training loss 0.7715220472696797\n",
            "2022-03-30 16:47:27.784065 Epoch 62, Training loss 0.7694867578385126\n",
            "2022-03-30 16:47:43.676815 Epoch 63, Training loss 0.7668075051225359\n",
            "2022-03-30 16:47:59.550490 Epoch 64, Training loss 0.7630550314855697\n",
            "2022-03-30 16:48:15.429436 Epoch 65, Training loss 0.7617512677255494\n",
            "2022-03-30 16:48:31.453148 Epoch 66, Training loss 0.7578903200757473\n",
            "2022-03-30 16:48:47.422309 Epoch 67, Training loss 0.7552895885523018\n",
            "2022-03-30 16:49:03.542129 Epoch 68, Training loss 0.7537719225106032\n",
            "2022-03-30 16:49:19.659124 Epoch 69, Training loss 0.7505699963215977\n",
            "2022-03-30 16:49:35.642250 Epoch 70, Training loss 0.7487169755694202\n",
            "2022-03-30 16:49:51.562980 Epoch 71, Training loss 0.7443486408275717\n",
            "2022-03-30 16:50:07.521308 Epoch 72, Training loss 0.7415367709782422\n",
            "2022-03-30 16:50:23.425481 Epoch 73, Training loss 0.7410154811790227\n",
            "2022-03-30 16:50:39.239368 Epoch 74, Training loss 0.7385134589870263\n",
            "2022-03-30 16:50:55.101521 Epoch 75, Training loss 0.7362193904264503\n",
            "2022-03-30 16:51:10.934053 Epoch 76, Training loss 0.7325823268926966\n",
            "2022-03-30 16:51:26.720232 Epoch 77, Training loss 0.7323704719009911\n",
            "2022-03-30 16:51:42.609685 Epoch 78, Training loss 0.7299186111716054\n",
            "2022-03-30 16:51:58.556548 Epoch 79, Training loss 0.7281802478806138\n",
            "2022-03-30 16:52:14.463647 Epoch 80, Training loss 0.7248785717560507\n",
            "2022-03-30 16:52:30.305720 Epoch 81, Training loss 0.7230445255175271\n",
            "2022-03-30 16:52:46.149986 Epoch 82, Training loss 0.7219327910019614\n",
            "2022-03-30 16:53:02.046352 Epoch 83, Training loss 0.718023071279916\n",
            "2022-03-30 16:53:17.857068 Epoch 84, Training loss 0.7165060482747719\n",
            "2022-03-30 16:53:33.780689 Epoch 85, Training loss 0.7146801852128085\n",
            "2022-03-30 16:53:49.631069 Epoch 86, Training loss 0.7130698169512517\n",
            "2022-03-30 16:54:05.409983 Epoch 87, Training loss 0.7099399144387306\n",
            "2022-03-30 16:54:21.199181 Epoch 88, Training loss 0.7072423534167697\n",
            "2022-03-30 16:54:36.992905 Epoch 89, Training loss 0.7072244490427739\n",
            "2022-03-30 16:54:52.903557 Epoch 90, Training loss 0.7059713099008936\n",
            "2022-03-30 16:55:08.448393 Epoch 91, Training loss 0.7039140535666205\n",
            "2022-03-30 16:55:24.044238 Epoch 92, Training loss 0.7001177923911063\n",
            "2022-03-30 16:55:39.514980 Epoch 93, Training loss 0.7012789372135612\n",
            "2022-03-30 16:55:55.033714 Epoch 94, Training loss 0.6961523350852225\n",
            "2022-03-30 16:56:10.553983 Epoch 95, Training loss 0.6961150652231158\n",
            "2022-03-30 16:56:26.113292 Epoch 96, Training loss 0.694989084930676\n",
            "2022-03-30 16:56:41.593311 Epoch 97, Training loss 0.693168774399611\n",
            "2022-03-30 16:56:56.922056 Epoch 98, Training loss 0.6890550130011176\n",
            "2022-03-30 16:57:12.210865 Epoch 99, Training loss 0.6881213087178862\n",
            "2022-03-30 16:57:27.762894 Epoch 100, Training loss 0.687820596348904\n",
            "2022-03-30 16:57:43.378614 Epoch 101, Training loss 0.68728623651635\n",
            "2022-03-30 16:57:58.959331 Epoch 102, Training loss 0.6854437130415226\n",
            "2022-03-30 16:58:14.606273 Epoch 103, Training loss 0.6814620846601398\n",
            "2022-03-30 16:58:30.279617 Epoch 104, Training loss 0.6811775289990408\n",
            "2022-03-30 16:58:45.912264 Epoch 105, Training loss 0.680617580023568\n",
            "2022-03-30 16:59:01.759846 Epoch 106, Training loss 0.6786834964590609\n",
            "2022-03-30 16:59:17.510326 Epoch 107, Training loss 0.6769724447099145\n",
            "2022-03-30 16:59:33.299108 Epoch 108, Training loss 0.6736122996682097\n",
            "2022-03-30 16:59:48.954475 Epoch 109, Training loss 0.6731095638726373\n",
            "2022-03-30 17:00:04.616357 Epoch 110, Training loss 0.6719475722755007\n",
            "2022-03-30 17:00:20.152231 Epoch 111, Training loss 0.6700799137811222\n",
            "2022-03-30 17:00:35.720469 Epoch 112, Training loss 0.6677608376040178\n",
            "2022-03-30 17:00:51.350404 Epoch 113, Training loss 0.6657814784809146\n",
            "2022-03-30 17:01:06.970653 Epoch 114, Training loss 0.6655398371731839\n",
            "2022-03-30 17:01:22.543724 Epoch 115, Training loss 0.6645731504844583\n",
            "2022-03-30 17:01:38.151482 Epoch 116, Training loss 0.6627267570523045\n",
            "2022-03-30 17:01:53.682506 Epoch 117, Training loss 0.6611453096961122\n",
            "2022-03-30 17:02:09.243197 Epoch 118, Training loss 0.6608886413866907\n",
            "2022-03-30 17:02:24.824462 Epoch 119, Training loss 0.6581175426006927\n",
            "2022-03-30 17:02:40.390529 Epoch 120, Training loss 0.6570441000296942\n",
            "2022-03-30 17:02:55.961717 Epoch 121, Training loss 0.6539637245180662\n",
            "2022-03-30 17:03:11.457195 Epoch 122, Training loss 0.6543380860096354\n",
            "2022-03-30 17:03:27.018943 Epoch 123, Training loss 0.6533657845557498\n",
            "2022-03-30 17:03:42.577026 Epoch 124, Training loss 0.651408141836181\n",
            "2022-03-30 17:03:58.176822 Epoch 125, Training loss 0.650339722252258\n",
            "2022-03-30 17:04:13.678312 Epoch 126, Training loss 0.6480506299554235\n",
            "2022-03-30 17:04:29.162498 Epoch 127, Training loss 0.6474758311153372\n",
            "2022-03-30 17:04:44.771924 Epoch 128, Training loss 0.6464485634318398\n",
            "2022-03-30 17:05:00.236602 Epoch 129, Training loss 0.6450290788165138\n",
            "2022-03-30 17:05:15.710711 Epoch 130, Training loss 0.6457167283043532\n",
            "2022-03-30 17:05:31.153973 Epoch 131, Training loss 0.6430513636611611\n",
            "2022-03-30 17:05:46.680532 Epoch 132, Training loss 0.6419495599120474\n",
            "2022-03-30 17:06:02.161176 Epoch 133, Training loss 0.64078714936743\n",
            "2022-03-30 17:06:17.651246 Epoch 134, Training loss 0.6398469922335251\n",
            "2022-03-30 17:06:33.271963 Epoch 135, Training loss 0.6379588383924016\n",
            "2022-03-30 17:06:48.778442 Epoch 136, Training loss 0.6365163458125366\n",
            "2022-03-30 17:07:04.344725 Epoch 137, Training loss 0.6349373391217283\n",
            "2022-03-30 17:07:19.794096 Epoch 138, Training loss 0.6345990595534025\n",
            "2022-03-30 17:07:35.431280 Epoch 139, Training loss 0.6333531581837198\n",
            "2022-03-30 17:07:51.054455 Epoch 140, Training loss 0.6341260188757001\n",
            "2022-03-30 17:08:06.662661 Epoch 141, Training loss 0.6312580229833608\n",
            "2022-03-30 17:08:22.214660 Epoch 142, Training loss 0.6300331497436289\n",
            "2022-03-30 17:08:37.732714 Epoch 143, Training loss 0.6308146272321491\n",
            "2022-03-30 17:08:53.166847 Epoch 144, Training loss 0.6269360880565156\n",
            "2022-03-30 17:09:08.651399 Epoch 145, Training loss 0.6271991705726785\n",
            "2022-03-30 17:09:24.179680 Epoch 146, Training loss 0.6268724898243194\n",
            "2022-03-30 17:09:39.659497 Epoch 147, Training loss 0.626205946325951\n",
            "2022-03-30 17:09:55.110023 Epoch 148, Training loss 0.6218528797102096\n",
            "2022-03-30 17:10:10.579373 Epoch 149, Training loss 0.6220394406858307\n",
            "2022-03-30 17:10:26.155804 Epoch 150, Training loss 0.6202104707508136\n",
            "2022-03-30 17:10:41.646195 Epoch 151, Training loss 0.6212231481776518\n",
            "2022-03-30 17:10:57.099043 Epoch 152, Training loss 0.620424222648906\n",
            "2022-03-30 17:11:12.608755 Epoch 153, Training loss 0.6182712639689141\n",
            "2022-03-30 17:11:28.136670 Epoch 154, Training loss 0.6186374833288095\n",
            "2022-03-30 17:11:43.587849 Epoch 155, Training loss 0.6159527077318152\n",
            "2022-03-30 17:11:59.078687 Epoch 156, Training loss 0.6159757954995041\n",
            "2022-03-30 17:12:14.602780 Epoch 157, Training loss 0.6140862061925556\n",
            "2022-03-30 17:12:30.072025 Epoch 158, Training loss 0.6130327208496421\n",
            "2022-03-30 17:12:45.521174 Epoch 159, Training loss 0.6127745842613528\n",
            "2022-03-30 17:13:00.958871 Epoch 160, Training loss 0.6113844737982201\n",
            "2022-03-30 17:13:16.404761 Epoch 161, Training loss 0.6097694554597216\n",
            "2022-03-30 17:13:31.942275 Epoch 162, Training loss 0.6093265687108345\n",
            "2022-03-30 17:13:47.417098 Epoch 163, Training loss 0.6096162804404793\n",
            "2022-03-30 17:14:02.842784 Epoch 164, Training loss 0.6069763084239972\n",
            "2022-03-30 17:14:18.305562 Epoch 165, Training loss 0.6059846025522407\n",
            "2022-03-30 17:14:33.834409 Epoch 166, Training loss 0.6054627491004022\n",
            "2022-03-30 17:14:49.409706 Epoch 167, Training loss 0.6042434245043093\n",
            "2022-03-30 17:15:04.872401 Epoch 168, Training loss 0.6049922395240316\n",
            "2022-03-30 17:15:20.428029 Epoch 169, Training loss 0.6024795754257676\n",
            "2022-03-30 17:15:36.194905 Epoch 170, Training loss 0.6022259109008038\n",
            "2022-03-30 17:15:51.981562 Epoch 171, Training loss 0.5994832227602029\n",
            "2022-03-30 17:16:07.759802 Epoch 172, Training loss 0.5997434297714697\n",
            "2022-03-30 17:16:23.559073 Epoch 173, Training loss 0.5989844161836083\n",
            "2022-03-30 17:16:39.121772 Epoch 174, Training loss 0.5987619335770302\n",
            "2022-03-30 17:16:54.669444 Epoch 175, Training loss 0.5952501678863145\n",
            "2022-03-30 17:17:10.199679 Epoch 176, Training loss 0.5970771955254742\n",
            "2022-03-30 17:17:25.717829 Epoch 177, Training loss 0.5951261108031358\n",
            "2022-03-30 17:17:41.280404 Epoch 178, Training loss 0.5961691672768434\n",
            "2022-03-30 17:17:56.901872 Epoch 179, Training loss 0.5923509488782615\n",
            "2022-03-30 17:18:12.479441 Epoch 180, Training loss 0.5939153361198543\n",
            "2022-03-30 17:18:27.970541 Epoch 181, Training loss 0.593285024737763\n",
            "2022-03-30 17:18:43.498824 Epoch 182, Training loss 0.5915564005941991\n",
            "2022-03-30 17:18:59.073900 Epoch 183, Training loss 0.5918215363074446\n",
            "2022-03-30 17:19:14.646962 Epoch 184, Training loss 0.5909150680693824\n",
            "2022-03-30 17:19:30.222993 Epoch 185, Training loss 0.5887920224605618\n",
            "2022-03-30 17:19:45.809486 Epoch 186, Training loss 0.5886753403088626\n",
            "2022-03-30 17:20:01.489777 Epoch 187, Training loss 0.5862397905200949\n",
            "2022-03-30 17:20:17.408653 Epoch 188, Training loss 0.5882560734248832\n",
            "2022-03-30 17:20:33.360724 Epoch 189, Training loss 0.5878983376657262\n",
            "2022-03-30 17:20:49.180515 Epoch 190, Training loss 0.5852352815592076\n",
            "2022-03-30 17:21:04.758174 Epoch 191, Training loss 0.5844417906859342\n",
            "2022-03-30 17:21:20.300776 Epoch 192, Training loss 0.5825317892843805\n",
            "2022-03-30 17:21:35.795881 Epoch 193, Training loss 0.5818182308122021\n",
            "2022-03-30 17:21:51.409124 Epoch 194, Training loss 0.5831658920973463\n",
            "2022-03-30 17:22:06.895120 Epoch 195, Training loss 0.580712525969576\n",
            "2022-03-30 17:22:22.467441 Epoch 196, Training loss 0.5797941839260519\n",
            "2022-03-30 17:22:37.977762 Epoch 197, Training loss 0.5784697068850403\n",
            "2022-03-30 17:22:53.552733 Epoch 198, Training loss 0.5797298571566487\n",
            "2022-03-30 17:23:09.031564 Epoch 199, Training loss 0.5781708735105632\n",
            "2022-03-30 17:23:24.576683 Epoch 200, Training loss 0.5775413541766383\n",
            "2022-03-30 17:23:40.237798 Epoch 201, Training loss 0.5761943906545639\n",
            "2022-03-30 17:23:55.855096 Epoch 202, Training loss 0.575277777431566\n",
            "2022-03-30 17:24:11.422164 Epoch 203, Training loss 0.5738115426905624\n",
            "2022-03-30 17:24:26.947824 Epoch 204, Training loss 0.5741367316840554\n",
            "2022-03-30 17:24:42.439337 Epoch 205, Training loss 0.5724221826971644\n",
            "2022-03-30 17:24:57.983608 Epoch 206, Training loss 0.5744029730558395\n",
            "2022-03-30 17:25:13.381012 Epoch 207, Training loss 0.5725554848647179\n",
            "2022-03-30 17:25:28.814408 Epoch 208, Training loss 0.5741474735919777\n",
            "2022-03-30 17:25:44.282823 Epoch 209, Training loss 0.5704494351926057\n",
            "2022-03-30 17:25:59.775470 Epoch 210, Training loss 0.5703181268842629\n",
            "2022-03-30 17:26:15.140950 Epoch 211, Training loss 0.5710417261285246\n",
            "2022-03-30 17:26:30.455839 Epoch 212, Training loss 0.5682413253714057\n",
            "2022-03-30 17:26:45.695920 Epoch 213, Training loss 0.5679558657700449\n",
            "2022-03-30 17:27:00.879761 Epoch 214, Training loss 0.5684952794209771\n",
            "2022-03-30 17:27:16.222310 Epoch 215, Training loss 0.5674158254700243\n",
            "2022-03-30 17:27:31.562348 Epoch 216, Training loss 0.5671424578370341\n",
            "2022-03-30 17:27:47.003412 Epoch 217, Training loss 0.5647368457006372\n",
            "2022-03-30 17:28:02.351749 Epoch 218, Training loss 0.5655340359872564\n",
            "2022-03-30 17:28:18.163498 Epoch 219, Training loss 0.5636106710833357\n",
            "2022-03-30 17:28:33.703084 Epoch 220, Training loss 0.5626170017835125\n",
            "2022-03-30 17:28:49.434954 Epoch 221, Training loss 0.562902580708494\n",
            "2022-03-30 17:29:04.942584 Epoch 222, Training loss 0.5617488363133673\n",
            "2022-03-30 17:29:20.359184 Epoch 223, Training loss 0.5605506423641654\n",
            "2022-03-30 17:29:35.739738 Epoch 224, Training loss 0.5632436937078491\n",
            "2022-03-30 17:29:51.129172 Epoch 225, Training loss 0.5604701635935118\n",
            "2022-03-30 17:30:06.509377 Epoch 226, Training loss 0.5586013966204261\n",
            "2022-03-30 17:30:21.880512 Epoch 227, Training loss 0.5606595343717223\n",
            "2022-03-30 17:30:37.232345 Epoch 228, Training loss 0.5586213894436122\n",
            "2022-03-30 17:30:52.618866 Epoch 229, Training loss 0.5578816822727622\n",
            "2022-03-30 17:31:08.079639 Epoch 230, Training loss 0.5572576090655363\n",
            "2022-03-30 17:31:23.458321 Epoch 231, Training loss 0.5574027275490334\n",
            "2022-03-30 17:31:38.875382 Epoch 232, Training loss 0.5563005194106065\n",
            "2022-03-30 17:31:54.361299 Epoch 233, Training loss 0.5548833052025122\n",
            "2022-03-30 17:32:09.743413 Epoch 234, Training loss 0.5545658432233059\n",
            "2022-03-30 17:32:25.132178 Epoch 235, Training loss 0.5554028235928482\n",
            "2022-03-30 17:32:40.449793 Epoch 236, Training loss 0.5534806968763356\n",
            "2022-03-30 17:32:55.836468 Epoch 237, Training loss 0.5526648356634027\n",
            "2022-03-30 17:33:11.194285 Epoch 238, Training loss 0.5537393775666156\n",
            "2022-03-30 17:33:26.573918 Epoch 239, Training loss 0.552850374518453\n",
            "2022-03-30 17:33:41.918859 Epoch 240, Training loss 0.5530139971572114\n",
            "2022-03-30 17:33:57.294642 Epoch 241, Training loss 0.551579137249371\n",
            "2022-03-30 17:34:12.497179 Epoch 242, Training loss 0.5508043096422235\n",
            "2022-03-30 17:34:27.747975 Epoch 243, Training loss 0.5494006165610555\n",
            "2022-03-30 17:34:42.942276 Epoch 244, Training loss 0.5489878907914052\n",
            "2022-03-30 17:34:58.471278 Epoch 245, Training loss 0.5473369641224747\n",
            "2022-03-30 17:35:13.836999 Epoch 246, Training loss 0.5486927178433484\n",
            "2022-03-30 17:35:29.187310 Epoch 247, Training loss 0.5497159306579233\n",
            "2022-03-30 17:35:44.657186 Epoch 248, Training loss 0.5470014163828871\n",
            "2022-03-30 17:35:59.998745 Epoch 249, Training loss 0.5459186157111622\n",
            "2022-03-30 17:36:15.360655 Epoch 250, Training loss 0.545278146100776\n",
            "2022-03-30 17:36:30.720562 Epoch 251, Training loss 0.5463507465084495\n",
            "2022-03-30 17:36:45.984066 Epoch 252, Training loss 0.5452621874906828\n",
            "2022-03-30 17:37:01.137666 Epoch 253, Training loss 0.5441234520710337\n",
            "2022-03-30 17:37:16.265827 Epoch 254, Training loss 0.5442825056174222\n",
            "2022-03-30 17:37:31.383686 Epoch 255, Training loss 0.5418479339317288\n",
            "2022-03-30 17:37:46.513151 Epoch 256, Training loss 0.5436948140144653\n",
            "2022-03-30 17:38:01.647121 Epoch 257, Training loss 0.5407581662811587\n",
            "2022-03-30 17:38:16.832004 Epoch 258, Training loss 0.5421689675973199\n",
            "2022-03-30 17:38:32.459385 Epoch 259, Training loss 0.5412541539848917\n",
            "2022-03-30 17:38:47.996527 Epoch 260, Training loss 0.5405552759957131\n",
            "2022-03-30 17:39:03.680738 Epoch 261, Training loss 0.539589646279507\n",
            "2022-03-30 17:39:19.216417 Epoch 262, Training loss 0.5394045747721287\n",
            "2022-03-30 17:39:34.607914 Epoch 263, Training loss 0.5382738555102702\n",
            "2022-03-30 17:39:49.929315 Epoch 264, Training loss 0.5412106632881457\n",
            "2022-03-30 17:40:05.247287 Epoch 265, Training loss 0.538946170617095\n",
            "2022-03-30 17:40:20.418918 Epoch 266, Training loss 0.5374490423199466\n",
            "2022-03-30 17:40:35.630945 Epoch 267, Training loss 0.5372481646821322\n",
            "2022-03-30 17:40:51.008094 Epoch 268, Training loss 0.5389455264181737\n",
            "2022-03-30 17:41:06.380072 Epoch 269, Training loss 0.5361159965967583\n",
            "2022-03-30 17:41:21.738490 Epoch 270, Training loss 0.5367615752665283\n",
            "2022-03-30 17:41:36.977615 Epoch 271, Training loss 0.537127623167794\n",
            "2022-03-30 17:41:52.412863 Epoch 272, Training loss 0.5361833616405192\n",
            "2022-03-30 17:42:07.907780 Epoch 273, Training loss 0.535734402211121\n",
            "2022-03-30 17:42:23.427907 Epoch 274, Training loss 0.5352916834909288\n",
            "2022-03-30 17:42:38.925167 Epoch 275, Training loss 0.5344431940704355\n",
            "2022-03-30 17:42:54.427581 Epoch 276, Training loss 0.5343168413700046\n",
            "2022-03-30 17:43:09.978496 Epoch 277, Training loss 0.5317246761468365\n",
            "2022-03-30 17:43:25.462560 Epoch 278, Training loss 0.5321196833306261\n",
            "2022-03-30 17:43:40.889312 Epoch 279, Training loss 0.5330367717139252\n",
            "2022-03-30 17:43:56.763938 Epoch 280, Training loss 0.5303812967541882\n",
            "2022-03-30 17:44:12.473096 Epoch 281, Training loss 0.5326643167706706\n",
            "2022-03-30 17:44:28.248765 Epoch 282, Training loss 0.5302402901527522\n",
            "2022-03-30 17:44:43.806983 Epoch 283, Training loss 0.5286450705984059\n",
            "2022-03-30 17:44:59.340777 Epoch 284, Training loss 0.5294832676039327\n",
            "2022-03-30 17:45:14.857476 Epoch 285, Training loss 0.527727341312734\n",
            "2022-03-30 17:45:30.181477 Epoch 286, Training loss 0.528007064729243\n",
            "2022-03-30 17:45:45.573777 Epoch 287, Training loss 0.5282846264293432\n",
            "2022-03-30 17:46:00.948968 Epoch 288, Training loss 0.528542934457207\n",
            "2022-03-30 17:46:16.303853 Epoch 289, Training loss 0.5255128258024641\n",
            "2022-03-30 17:46:31.691698 Epoch 290, Training loss 0.5260909891418178\n",
            "2022-03-30 17:46:47.043056 Epoch 291, Training loss 0.5257655900457631\n",
            "2022-03-30 17:47:02.295593 Epoch 292, Training loss 0.5262171456880886\n",
            "2022-03-30 17:47:17.658589 Epoch 293, Training loss 0.5266468902416241\n",
            "2022-03-30 17:47:33.034015 Epoch 294, Training loss 0.5275501125227765\n",
            "2022-03-30 17:47:48.458279 Epoch 295, Training loss 0.5250356104558386\n",
            "2022-03-30 17:48:03.948255 Epoch 296, Training loss 0.5240240449566975\n",
            "2022-03-30 17:48:19.577218 Epoch 297, Training loss 0.5223860949697092\n",
            "2022-03-30 17:48:35.097100 Epoch 298, Training loss 0.5237912364932887\n",
            "2022-03-30 17:48:50.712155 Epoch 299, Training loss 0.5244807316099896\n",
            "2022-03-30 17:49:06.350726 Epoch 300, Training loss 0.5235248209189272\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(),lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086988a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "086988a8",
        "outputId": "2bb7c3c0-ed1a-4912-a760-034c2c2f5daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.75\n",
            "Accuracy val: 0.60\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## Part 2 #############"
      ],
      "metadata": {
        "id": "LAseU4_cr4VC"
      },
      "id": "LAseU4_cr4VC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "            self.act1 = nn.Tanh()\n",
        "            self.pool1 = nn.MaxPool2d(2)\n",
        "            self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "            self.act2 = nn.Tanh()\n",
        "            self.pool2 = nn.MaxPool2d(2)\n",
        "            self.conv3 = nn.Conv2d(8, 3, kernel_size=3, padding=1)\n",
        "            self.act3 = nn.Tanh()\n",
        "            self.pool3 = nn.MaxPool2d(2)\n",
        "            self.fc1 = nn.Linear(8*8*8, 32)\n",
        "            self.act4 = nn.Tanh()\n",
        "            self.fc2 = nn.Linear(32,10)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            out = self.pool1(self.act1(self.conv1(x)))\n",
        "            out = self.pool2(self.act2(self.conv2(out)))\n",
        "            out = out.view(-1, 8*8*8)\n",
        "            out = self.act3(self.fc1(out))\n",
        "            out = self.fc2(out)\n",
        "            return out\n"
      ],
      "metadata": {
        "id": "sKOFYcwSr-_e"
      },
      "id": "sKOFYcwSr-_e",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(),lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXLqZw7qshTT",
        "outputId": "7754c600-2cff-4212-c2d4-305de7a24c0c"
      },
      "id": "qXLqZw7qshTT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 19:57:46.639845 Epoch 1, Training loss 2.0342248437349753\n",
            "2022-03-30 19:58:04.252934 Epoch 2, Training loss 1.752946306220101\n",
            "2022-03-30 19:58:21.964344 Epoch 3, Training loss 1.584032882814822\n",
            "2022-03-30 19:58:39.525112 Epoch 4, Training loss 1.4926144758148876\n",
            "2022-03-30 19:58:57.294147 Epoch 5, Training loss 1.423058059209448\n",
            "2022-03-30 19:59:14.961421 Epoch 6, Training loss 1.3588145898126276\n",
            "2022-03-30 19:59:32.613245 Epoch 7, Training loss 1.3056890267850187\n",
            "2022-03-30 19:59:50.221532 Epoch 8, Training loss 1.2642026870604366\n",
            "2022-03-30 20:00:07.791012 Epoch 9, Training loss 1.2260863840427545\n",
            "2022-03-30 20:00:25.309662 Epoch 10, Training loss 1.1931788312352223\n",
            "2022-03-30 20:00:42.751348 Epoch 11, Training loss 1.1648307405316922\n",
            "2022-03-30 20:01:00.266555 Epoch 12, Training loss 1.140716722859141\n",
            "2022-03-30 20:01:17.825681 Epoch 13, Training loss 1.1203773072765917\n",
            "2022-03-30 20:01:35.349738 Epoch 14, Training loss 1.1004196355867264\n",
            "2022-03-30 20:01:53.096633 Epoch 15, Training loss 1.0840247617963026\n",
            "2022-03-30 20:02:10.600165 Epoch 16, Training loss 1.0686080322393676\n",
            "2022-03-30 20:02:27.931365 Epoch 17, Training loss 1.0538585762233685\n",
            "2022-03-30 20:02:45.212827 Epoch 18, Training loss 1.039152557435243\n",
            "2022-03-30 20:03:02.695055 Epoch 19, Training loss 1.0273409199989056\n",
            "2022-03-30 20:03:20.046548 Epoch 20, Training loss 1.0156118160928302\n",
            "2022-03-30 20:03:37.460480 Epoch 21, Training loss 1.0008823745086064\n",
            "2022-03-30 20:03:54.974328 Epoch 22, Training loss 0.9901235699653625\n",
            "2022-03-30 20:04:12.313073 Epoch 23, Training loss 0.9793424905108674\n",
            "2022-03-30 20:04:29.851521 Epoch 24, Training loss 0.9700810456519846\n",
            "2022-03-30 20:04:47.409920 Epoch 25, Training loss 0.9589759558058151\n",
            "2022-03-30 20:05:04.820398 Epoch 26, Training loss 0.9511332992854935\n",
            "2022-03-30 20:05:22.253323 Epoch 27, Training loss 0.9404823715272157\n",
            "2022-03-30 20:05:39.621454 Epoch 28, Training loss 0.9338466160742523\n",
            "2022-03-30 20:05:56.915103 Epoch 29, Training loss 0.9247029037274364\n",
            "2022-03-30 20:06:14.328950 Epoch 30, Training loss 0.9167556162837827\n",
            "2022-03-30 20:06:31.627435 Epoch 31, Training loss 0.9108470457289225\n",
            "2022-03-30 20:06:49.091088 Epoch 32, Training loss 0.9028504704270521\n",
            "2022-03-30 20:07:06.441297 Epoch 33, Training loss 0.8962534906919045\n",
            "2022-03-30 20:07:23.714894 Epoch 34, Training loss 0.8894333268522912\n",
            "2022-03-30 20:07:40.979469 Epoch 35, Training loss 0.8834901117836423\n",
            "2022-03-30 20:07:58.256849 Epoch 36, Training loss 0.8771757272350819\n",
            "2022-03-30 20:08:15.406289 Epoch 37, Training loss 0.8697883135369976\n",
            "2022-03-30 20:08:32.732101 Epoch 38, Training loss 0.8657480033157426\n",
            "2022-03-30 20:08:50.021524 Epoch 39, Training loss 0.8603859867738641\n",
            "2022-03-30 20:09:07.402948 Epoch 40, Training loss 0.8569514326884619\n",
            "2022-03-30 20:09:24.818698 Epoch 41, Training loss 0.8510969571597741\n",
            "2022-03-30 20:09:42.204107 Epoch 42, Training loss 0.8456652215146043\n",
            "2022-03-30 20:09:59.583103 Epoch 43, Training loss 0.8417517423172436\n",
            "2022-03-30 20:10:16.936238 Epoch 44, Training loss 0.8369628666611888\n",
            "2022-03-30 20:10:34.197998 Epoch 45, Training loss 0.833328200994855\n",
            "2022-03-30 20:10:51.548334 Epoch 46, Training loss 0.8290103589710982\n",
            "2022-03-30 20:11:08.938779 Epoch 47, Training loss 0.8256764249576022\n",
            "2022-03-30 20:11:26.152286 Epoch 48, Training loss 0.8211713511010875\n",
            "2022-03-30 20:11:43.411634 Epoch 49, Training loss 0.8170627376536275\n",
            "2022-03-30 20:12:00.649897 Epoch 50, Training loss 0.81293194822948\n",
            "2022-03-30 20:12:17.935513 Epoch 51, Training loss 0.8088343027988663\n",
            "2022-03-30 20:12:35.209597 Epoch 52, Training loss 0.8068729293773241\n",
            "2022-03-30 20:12:52.561705 Epoch 53, Training loss 0.8023556878652109\n",
            "2022-03-30 20:13:09.822551 Epoch 54, Training loss 0.7995868367535989\n",
            "2022-03-30 20:13:27.137718 Epoch 55, Training loss 0.7963480365550731\n",
            "2022-03-30 20:13:44.429008 Epoch 56, Training loss 0.7932841290750772\n",
            "2022-03-30 20:14:01.734068 Epoch 57, Training loss 0.7909108110324806\n",
            "2022-03-30 20:14:19.069268 Epoch 58, Training loss 0.7853624482288994\n",
            "2022-03-30 20:14:36.307136 Epoch 59, Training loss 0.7841279197608113\n",
            "2022-03-30 20:14:53.617084 Epoch 60, Training loss 0.7813546390789549\n",
            "2022-03-30 20:15:10.799890 Epoch 61, Training loss 0.7771572351379468\n",
            "2022-03-30 20:15:27.961521 Epoch 62, Training loss 0.7758684234164864\n",
            "2022-03-30 20:15:45.216317 Epoch 63, Training loss 0.7724709667818016\n",
            "2022-03-30 20:16:02.553074 Epoch 64, Training loss 0.7695004771203946\n",
            "2022-03-30 20:16:19.793569 Epoch 65, Training loss 0.767278568519046\n",
            "2022-03-30 20:16:36.853986 Epoch 66, Training loss 0.7638452553078342\n",
            "2022-03-30 20:16:54.168687 Epoch 67, Training loss 0.7603750408762862\n",
            "2022-03-30 20:17:11.407930 Epoch 68, Training loss 0.7594792579522218\n",
            "2022-03-30 20:17:28.662067 Epoch 69, Training loss 0.755063569263729\n",
            "2022-03-30 20:17:45.921658 Epoch 70, Training loss 0.7540120729781172\n",
            "2022-03-30 20:18:03.240317 Epoch 71, Training loss 0.7526739356310471\n",
            "2022-03-30 20:18:20.458804 Epoch 72, Training loss 0.7495680485692475\n",
            "2022-03-30 20:18:37.542692 Epoch 73, Training loss 0.7463213972499608\n",
            "2022-03-30 20:18:54.740055 Epoch 74, Training loss 0.7459405917112175\n",
            "2022-03-30 20:19:11.865238 Epoch 75, Training loss 0.7434299854976137\n",
            "2022-03-30 20:19:29.023545 Epoch 76, Training loss 0.7388355324182974\n",
            "2022-03-30 20:19:46.315792 Epoch 77, Training loss 0.7371681176716715\n",
            "2022-03-30 20:20:03.590060 Epoch 78, Training loss 0.7344309442564655\n",
            "2022-03-30 20:20:20.782692 Epoch 79, Training loss 0.7339688417170663\n",
            "2022-03-30 20:20:37.849977 Epoch 80, Training loss 0.7304625836251032\n",
            "2022-03-30 20:20:54.965352 Epoch 81, Training loss 0.728321910399915\n",
            "2022-03-30 20:21:12.151006 Epoch 82, Training loss 0.7281297923582594\n",
            "2022-03-30 20:21:29.341322 Epoch 83, Training loss 0.7248115371102872\n",
            "2022-03-30 20:21:46.551122 Epoch 84, Training loss 0.722956552644215\n",
            "2022-03-30 20:22:03.755476 Epoch 85, Training loss 0.7208482356327574\n",
            "2022-03-30 20:22:20.900437 Epoch 86, Training loss 0.7177300063697883\n",
            "2022-03-30 20:22:38.081404 Epoch 87, Training loss 0.7173017699014196\n",
            "2022-03-30 20:22:55.423414 Epoch 88, Training loss 0.714791690792574\n",
            "2022-03-30 20:23:12.800350 Epoch 89, Training loss 0.7133188956152753\n",
            "2022-03-30 20:23:30.014114 Epoch 90, Training loss 0.7103960397069716\n",
            "2022-03-30 20:23:47.125313 Epoch 91, Training loss 0.7104774507720147\n",
            "2022-03-30 20:24:04.251498 Epoch 92, Training loss 0.7070116330595577\n",
            "2022-03-30 20:24:21.363333 Epoch 93, Training loss 0.7054232065482517\n",
            "2022-03-30 20:24:38.680254 Epoch 94, Training loss 0.7034177584645084\n",
            "2022-03-30 20:24:55.929691 Epoch 95, Training loss 0.7013568317188936\n",
            "2022-03-30 20:25:13.201224 Epoch 96, Training loss 0.7017883618774317\n",
            "2022-03-30 20:25:30.359621 Epoch 97, Training loss 0.6980414885236784\n",
            "2022-03-30 20:25:47.487615 Epoch 98, Training loss 0.6973674788194544\n",
            "2022-03-30 20:26:04.638590 Epoch 99, Training loss 0.6954033185377755\n",
            "2022-03-30 20:26:21.887118 Epoch 100, Training loss 0.6928628374777182\n",
            "2022-03-30 20:26:39.231751 Epoch 101, Training loss 0.6925161777783537\n",
            "2022-03-30 20:26:56.415817 Epoch 102, Training loss 0.6903624612733227\n",
            "2022-03-30 20:27:13.720396 Epoch 103, Training loss 0.689741833740488\n",
            "2022-03-30 20:27:31.099532 Epoch 104, Training loss 0.6876075844020795\n",
            "2022-03-30 20:27:48.382312 Epoch 105, Training loss 0.6857277073366258\n",
            "2022-03-30 20:28:05.749195 Epoch 106, Training loss 0.6844128517010023\n",
            "2022-03-30 20:28:23.101388 Epoch 107, Training loss 0.684342842044123\n",
            "2022-03-30 20:28:40.440448 Epoch 108, Training loss 0.681182753894945\n",
            "2022-03-30 20:28:57.824386 Epoch 109, Training loss 0.6789976147282154\n",
            "2022-03-30 20:29:15.137867 Epoch 110, Training loss 0.6784149662536734\n",
            "2022-03-30 20:29:32.438238 Epoch 111, Training loss 0.6779407529193727\n",
            "2022-03-30 20:29:49.857619 Epoch 112, Training loss 0.6769531208383458\n",
            "2022-03-30 20:30:07.192021 Epoch 113, Training loss 0.6742400948501304\n",
            "2022-03-30 20:30:24.513984 Epoch 114, Training loss 0.6737716463978028\n",
            "2022-03-30 20:30:41.781173 Epoch 115, Training loss 0.6706839475561591\n",
            "2022-03-30 20:30:59.202589 Epoch 116, Training loss 0.6710634443842237\n",
            "2022-03-30 20:31:16.501965 Epoch 117, Training loss 0.6686631716273325\n",
            "2022-03-30 20:31:33.820650 Epoch 118, Training loss 0.667027006978574\n",
            "2022-03-30 20:31:50.892024 Epoch 119, Training loss 0.6655785441779725\n",
            "2022-03-30 20:32:08.029461 Epoch 120, Training loss 0.6643407618069588\n",
            "2022-03-30 20:32:25.222637 Epoch 121, Training loss 0.6632043127818485\n",
            "2022-03-30 20:32:42.426185 Epoch 122, Training loss 0.6610526283988563\n",
            "2022-03-30 20:32:59.586829 Epoch 123, Training loss 0.6605237853115477\n",
            "2022-03-30 20:33:16.702985 Epoch 124, Training loss 0.6589441145472514\n",
            "2022-03-30 20:33:33.855648 Epoch 125, Training loss 0.6563516488236845\n",
            "2022-03-30 20:33:51.137099 Epoch 126, Training loss 0.6567810787188123\n",
            "2022-03-30 20:34:08.549387 Epoch 127, Training loss 0.6556540989433713\n",
            "2022-03-30 20:34:25.778613 Epoch 128, Training loss 0.6561180837547688\n",
            "2022-03-30 20:34:42.920560 Epoch 129, Training loss 0.6525601105159505\n",
            "2022-03-30 20:35:00.118691 Epoch 130, Training loss 0.6519742381313572\n",
            "2022-03-30 20:35:17.285501 Epoch 131, Training loss 0.6506298509476435\n",
            "2022-03-30 20:35:34.384655 Epoch 132, Training loss 0.648913685889805\n",
            "2022-03-30 20:35:51.485764 Epoch 133, Training loss 0.6489152517312627\n",
            "2022-03-30 20:36:08.641387 Epoch 134, Training loss 0.6476028255946801\n",
            "2022-03-30 20:36:25.913330 Epoch 135, Training loss 0.6446833660840379\n",
            "2022-03-30 20:36:43.076057 Epoch 136, Training loss 0.6457981913519637\n",
            "2022-03-30 20:37:00.224251 Epoch 137, Training loss 0.6435257539038768\n",
            "2022-03-30 20:37:17.311578 Epoch 138, Training loss 0.643059634224838\n",
            "2022-03-30 20:37:34.452896 Epoch 139, Training loss 0.6417391409197122\n",
            "2022-03-30 20:37:51.736279 Epoch 140, Training loss 0.6409995186969143\n",
            "2022-03-30 20:38:08.898133 Epoch 141, Training loss 0.6391803782309413\n",
            "2022-03-30 20:38:26.049309 Epoch 142, Training loss 0.6373071750564039\n",
            "2022-03-30 20:38:43.196284 Epoch 143, Training loss 0.6364168825433077\n",
            "2022-03-30 20:39:00.381161 Epoch 144, Training loss 0.6353421795856008\n",
            "2022-03-30 20:39:17.665110 Epoch 145, Training loss 0.6343196098456907\n",
            "2022-03-30 20:39:34.897577 Epoch 146, Training loss 0.6331672933324218\n",
            "2022-03-30 20:39:52.131288 Epoch 147, Training loss 0.6324421069429963\n",
            "2022-03-30 20:40:09.358399 Epoch 148, Training loss 0.6318614413899839\n",
            "2022-03-30 20:40:26.482448 Epoch 149, Training loss 0.6299046316110265\n",
            "2022-03-30 20:40:43.625570 Epoch 150, Training loss 0.627888308880884\n",
            "2022-03-30 20:41:00.776338 Epoch 151, Training loss 0.6275859218271796\n",
            "2022-03-30 20:41:17.995312 Epoch 152, Training loss 0.6283919117258637\n",
            "2022-03-30 20:41:35.084293 Epoch 153, Training loss 0.626870873257937\n",
            "2022-03-30 20:41:52.190542 Epoch 154, Training loss 0.6258844983242356\n",
            "2022-03-30 20:42:09.314073 Epoch 155, Training loss 0.6242655043483085\n",
            "2022-03-30 20:42:26.509121 Epoch 156, Training loss 0.6231887723936145\n",
            "2022-03-30 20:42:43.646161 Epoch 157, Training loss 0.6229495766293972\n",
            "2022-03-30 20:43:00.980842 Epoch 158, Training loss 0.6223323055926491\n",
            "2022-03-30 20:43:18.297135 Epoch 159, Training loss 0.6200679917164775\n",
            "2022-03-30 20:43:35.667329 Epoch 160, Training loss 0.6178688811295477\n",
            "2022-03-30 20:43:52.899680 Epoch 161, Training loss 0.618835144183215\n",
            "2022-03-30 20:44:10.157824 Epoch 162, Training loss 0.6177248711628682\n",
            "2022-03-30 20:44:27.408445 Epoch 163, Training loss 0.6171155705705018\n",
            "2022-03-30 20:44:44.534452 Epoch 164, Training loss 0.6167602711321448\n",
            "2022-03-30 20:45:01.727752 Epoch 165, Training loss 0.6153525577100647\n",
            "2022-03-30 20:45:18.900735 Epoch 166, Training loss 0.6136069425650875\n",
            "2022-03-30 20:45:36.105551 Epoch 167, Training loss 0.6130746884647843\n",
            "2022-03-30 20:45:53.349823 Epoch 168, Training loss 0.6123050497011151\n",
            "2022-03-30 20:46:10.574179 Epoch 169, Training loss 0.6113469550752884\n",
            "2022-03-30 20:46:27.852539 Epoch 170, Training loss 0.6098251911380407\n",
            "2022-03-30 20:46:45.106729 Epoch 171, Training loss 0.6084404421203277\n",
            "2022-03-30 20:47:02.388033 Epoch 172, Training loss 0.6078177636008129\n",
            "2022-03-30 20:47:19.543048 Epoch 173, Training loss 0.6075675249709498\n",
            "2022-03-30 20:47:36.854023 Epoch 174, Training loss 0.6055700741231899\n",
            "2022-03-30 20:47:54.106856 Epoch 175, Training loss 0.6055827689597674\n",
            "2022-03-30 20:48:11.238446 Epoch 176, Training loss 0.6047393036315508\n",
            "2022-03-30 20:48:28.463842 Epoch 177, Training loss 0.6042697837057016\n",
            "2022-03-30 20:48:45.628550 Epoch 178, Training loss 0.602694076650283\n",
            "2022-03-30 20:49:02.758415 Epoch 179, Training loss 0.6028183368618226\n",
            "2022-03-30 20:49:19.904305 Epoch 180, Training loss 0.6014110473034632\n",
            "2022-03-30 20:49:37.073317 Epoch 181, Training loss 0.5997264464874097\n",
            "2022-03-30 20:49:54.149672 Epoch 182, Training loss 0.5996115598303583\n",
            "2022-03-30 20:50:11.195829 Epoch 183, Training loss 0.5996305254642921\n",
            "2022-03-30 20:50:28.227576 Epoch 184, Training loss 0.5962603426802798\n",
            "2022-03-30 20:50:45.351344 Epoch 185, Training loss 0.5974782819257063\n",
            "2022-03-30 20:51:02.497751 Epoch 186, Training loss 0.5945489018240853\n",
            "2022-03-30 20:51:19.540126 Epoch 187, Training loss 0.5961564747268892\n",
            "2022-03-30 20:51:36.683692 Epoch 188, Training loss 0.5946013185450488\n",
            "2022-03-30 20:51:53.786615 Epoch 189, Training loss 0.5953940086428772\n",
            "2022-03-30 20:52:10.940302 Epoch 190, Training loss 0.5920210404088125\n",
            "2022-03-30 20:52:28.116731 Epoch 191, Training loss 0.5927706102428534\n",
            "2022-03-30 20:52:45.457159 Epoch 192, Training loss 0.5908902164005563\n",
            "2022-03-30 20:53:02.809355 Epoch 193, Training loss 0.590358570103755\n",
            "2022-03-30 20:53:20.111283 Epoch 194, Training loss 0.5878164650839003\n",
            "2022-03-30 20:53:37.430347 Epoch 195, Training loss 0.5890926087984953\n",
            "2022-03-30 20:53:54.677523 Epoch 196, Training loss 0.5886439863983017\n",
            "2022-03-30 20:54:11.922424 Epoch 197, Training loss 0.5858386495838994\n",
            "2022-03-30 20:54:29.205200 Epoch 198, Training loss 0.5869879004976634\n",
            "2022-03-30 20:54:46.317837 Epoch 199, Training loss 0.5869938902690283\n",
            "2022-03-30 20:55:03.482200 Epoch 200, Training loss 0.5843104399226206\n",
            "2022-03-30 20:55:20.749948 Epoch 201, Training loss 0.5840351395975903\n",
            "2022-03-30 20:55:37.922507 Epoch 202, Training loss 0.5829792844746119\n",
            "2022-03-30 20:55:55.064038 Epoch 203, Training loss 0.5837632152811646\n",
            "2022-03-30 20:56:12.203519 Epoch 204, Training loss 0.5813446661929036\n",
            "2022-03-30 20:56:29.355838 Epoch 205, Training loss 0.5808051725863801\n",
            "2022-03-30 20:56:46.514709 Epoch 206, Training loss 0.5808494374575213\n",
            "2022-03-30 20:57:03.719333 Epoch 207, Training loss 0.5810563598790437\n",
            "2022-03-30 20:57:20.867980 Epoch 208, Training loss 0.5790569552832552\n",
            "2022-03-30 20:57:38.150751 Epoch 209, Training loss 0.578879052492054\n",
            "2022-03-30 20:57:55.504396 Epoch 210, Training loss 0.5784160869429483\n",
            "2022-03-30 20:58:12.810955 Epoch 211, Training loss 0.577803978377291\n",
            "2022-03-30 20:58:30.052614 Epoch 212, Training loss 0.5764303334305049\n",
            "2022-03-30 20:58:47.258682 Epoch 213, Training loss 0.576668894580563\n",
            "2022-03-30 20:59:04.408392 Epoch 214, Training loss 0.5746780720818073\n",
            "2022-03-30 20:59:21.564422 Epoch 215, Training loss 0.5741989521495522\n",
            "2022-03-30 20:59:38.851418 Epoch 216, Training loss 0.5741691150323814\n",
            "2022-03-30 20:59:56.019025 Epoch 217, Training loss 0.5728626141462789\n",
            "2022-03-30 21:00:13.228781 Epoch 218, Training loss 0.572730872179846\n",
            "2022-03-30 21:00:30.492466 Epoch 219, Training loss 0.5719148441577506\n",
            "2022-03-30 21:00:47.688007 Epoch 220, Training loss 0.5705864753793267\n",
            "2022-03-30 21:01:04.884334 Epoch 221, Training loss 0.5705722139009735\n",
            "2022-03-30 21:01:22.046423 Epoch 222, Training loss 0.5707227033193764\n",
            "2022-03-30 21:01:39.251353 Epoch 223, Training loss 0.5681399214069557\n",
            "2022-03-30 21:01:56.499779 Epoch 224, Training loss 0.5695425922151112\n",
            "2022-03-30 21:02:13.833759 Epoch 225, Training loss 0.5682261630778422\n",
            "2022-03-30 21:02:31.076698 Epoch 226, Training loss 0.5681413194102705\n",
            "2022-03-30 21:02:48.330961 Epoch 227, Training loss 0.5655801823681883\n",
            "2022-03-30 21:03:05.414956 Epoch 228, Training loss 0.5659799655456372\n",
            "2022-03-30 21:03:22.442842 Epoch 229, Training loss 0.56470099796572\n",
            "2022-03-30 21:03:39.545772 Epoch 230, Training loss 0.5636576299396012\n",
            "2022-03-30 21:03:56.590832 Epoch 231, Training loss 0.562755445461444\n",
            "2022-03-30 21:04:13.535410 Epoch 232, Training loss 0.5644848195221418\n",
            "2022-03-30 21:04:30.485215 Epoch 233, Training loss 0.5629554960276465\n",
            "2022-03-30 21:04:47.489839 Epoch 234, Training loss 0.5626271589256614\n",
            "2022-03-30 21:05:04.507229 Epoch 235, Training loss 0.5626347097365753\n",
            "2022-03-30 21:05:21.531812 Epoch 236, Training loss 0.5607352732773632\n",
            "2022-03-30 21:05:38.487531 Epoch 237, Training loss 0.5604396356493616\n",
            "2022-03-30 21:05:55.445122 Epoch 238, Training loss 0.5614003816910107\n",
            "2022-03-30 21:06:12.397321 Epoch 239, Training loss 0.5576061383842508\n",
            "2022-03-30 21:06:29.373443 Epoch 240, Training loss 0.5599452159212678\n",
            "2022-03-30 21:06:46.389276 Epoch 241, Training loss 0.557871897240429\n",
            "2022-03-30 21:07:03.348688 Epoch 242, Training loss 0.5582512538222706\n",
            "2022-03-30 21:07:20.290035 Epoch 243, Training loss 0.5570022103464817\n",
            "2022-03-30 21:07:37.233116 Epoch 244, Training loss 0.556773970689615\n",
            "2022-03-30 21:07:54.163740 Epoch 245, Training loss 0.5577382735355431\n",
            "2022-03-30 21:08:11.006172 Epoch 246, Training loss 0.5559821151901999\n",
            "2022-03-30 21:08:27.917771 Epoch 247, Training loss 0.555764982157656\n",
            "2022-03-30 21:08:45.014878 Epoch 248, Training loss 0.5536739460723784\n",
            "2022-03-30 21:09:02.254586 Epoch 249, Training loss 0.5548988395487257\n",
            "2022-03-30 21:09:19.365116 Epoch 250, Training loss 0.5542233130130012\n",
            "2022-03-30 21:09:36.543875 Epoch 251, Training loss 0.5523836990756452\n",
            "2022-03-30 21:09:53.585410 Epoch 252, Training loss 0.5516126146897331\n",
            "2022-03-30 21:10:10.860861 Epoch 253, Training loss 0.5524438609704947\n",
            "2022-03-30 21:10:27.938319 Epoch 254, Training loss 0.5518311819304591\n",
            "2022-03-30 21:10:44.970880 Epoch 255, Training loss 0.5501300082792102\n",
            "2022-03-30 21:11:01.997720 Epoch 256, Training loss 0.550467735246929\n",
            "2022-03-30 21:11:19.044213 Epoch 257, Training loss 0.5487787662564642\n",
            "2022-03-30 21:11:36.188828 Epoch 258, Training loss 0.5482078108107648\n",
            "2022-03-30 21:11:53.402228 Epoch 259, Training loss 0.5489846072004884\n",
            "2022-03-30 21:12:10.804556 Epoch 260, Training loss 0.5496709937672786\n",
            "2022-03-30 21:12:28.051132 Epoch 261, Training loss 0.5470231935153227\n",
            "2022-03-30 21:12:45.310141 Epoch 262, Training loss 0.5472648377194429\n",
            "2022-03-30 21:13:02.491951 Epoch 263, Training loss 0.5455478313938736\n",
            "2022-03-30 21:13:19.546672 Epoch 264, Training loss 0.5465174364997908\n",
            "2022-03-30 21:13:36.587211 Epoch 265, Training loss 0.5459359771836444\n",
            "2022-03-30 21:13:53.713672 Epoch 266, Training loss 0.5454561456923595\n",
            "2022-03-30 21:14:10.825343 Epoch 267, Training loss 0.5457697181826662\n",
            "2022-03-30 21:14:27.903571 Epoch 268, Training loss 0.5443289530513536\n",
            "2022-03-30 21:14:44.998359 Epoch 269, Training loss 0.5428490629586418\n",
            "2022-03-30 21:15:02.126511 Epoch 270, Training loss 0.5421765322041938\n",
            "2022-03-30 21:15:19.199969 Epoch 271, Training loss 0.5413042746693887\n",
            "2022-03-30 21:15:36.541389 Epoch 272, Training loss 0.5421802926703793\n",
            "2022-03-30 21:15:53.975254 Epoch 273, Training loss 0.5421725409033963\n",
            "2022-03-30 21:16:11.206072 Epoch 274, Training loss 0.5425112615994481\n",
            "2022-03-30 21:16:28.276026 Epoch 275, Training loss 0.5402713044144004\n",
            "2022-03-30 21:16:45.689733 Epoch 276, Training loss 0.5409728109531695\n",
            "2022-03-30 21:17:03.008768 Epoch 277, Training loss 0.5407311642337638\n",
            "2022-03-30 21:17:20.328239 Epoch 278, Training loss 0.5370640200574684\n",
            "2022-03-30 21:17:37.621118 Epoch 279, Training loss 0.5392632898695938\n",
            "2022-03-30 21:17:55.067662 Epoch 280, Training loss 0.5397577440494772\n",
            "2022-03-30 21:18:12.428111 Epoch 281, Training loss 0.5392175502789295\n",
            "2022-03-30 21:18:29.666862 Epoch 282, Training loss 0.5378521254567235\n",
            "2022-03-30 21:18:46.954857 Epoch 283, Training loss 0.537983384259674\n",
            "2022-03-30 21:19:04.266303 Epoch 284, Training loss 0.537645156521474\n",
            "2022-03-30 21:19:21.538594 Epoch 285, Training loss 0.5347873795672756\n",
            "2022-03-30 21:19:38.833350 Epoch 286, Training loss 0.536509883106517\n",
            "2022-03-30 21:19:56.197993 Epoch 287, Training loss 0.5351783700306397\n",
            "2022-03-30 21:20:13.476540 Epoch 288, Training loss 0.5344123496957447\n",
            "2022-03-30 21:20:30.839801 Epoch 289, Training loss 0.5358749100238162\n",
            "2022-03-30 21:20:48.251004 Epoch 290, Training loss 0.5358750258412812\n",
            "2022-03-30 21:21:05.562001 Epoch 291, Training loss 0.5346361612877273\n",
            "2022-03-30 21:21:22.832680 Epoch 292, Training loss 0.5338554907104244\n",
            "2022-03-30 21:21:40.204609 Epoch 293, Training loss 0.5326665434462335\n",
            "2022-03-30 21:21:57.394647 Epoch 294, Training loss 0.533229340182241\n",
            "2022-03-30 21:22:14.564767 Epoch 295, Training loss 0.5321442033438122\n",
            "2022-03-30 21:22:31.652875 Epoch 296, Training loss 0.5319240896217049\n",
            "2022-03-30 21:22:48.736949 Epoch 297, Training loss 0.5310009437067734\n",
            "2022-03-30 21:23:05.845457 Epoch 298, Training loss 0.5311518533684104\n",
            "2022-03-30 21:23:23.054070 Epoch 299, Training loss 0.5321260437636119\n",
            "2022-03-30 21:23:40.161401 Epoch 300, Training loss 0.5308179359911652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmssbCGZskLy",
        "outputId": "3c7a82e4-5e82-4ed5-d9f9-fa06a22b300c"
      },
      "id": "KmssbCGZskLy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########Problem 2###########"
      ],
      "metadata": {
        "id": "KiqlUWmRHoGx"
      },
      "id": "KiqlUWmRHoGx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########Part 1 #############"
      ],
      "metadata": {
        "id": "n0E4_mQXHwIo"
      },
      "id": "n0E4_mQXHwIo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "NXUKhck9HzSX"
      },
      "id": "NXUKhck9HzSX",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ss1RggRIBjd",
        "outputId": "a129a0bb-4654-4345-f287-c24f2097811b"
      },
      "id": "6Ss1RggRIBjd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 21:25:09.895866 Epoch 1, Training loss 2.064734585297382\n",
            "2022-03-30 21:25:30.762045 Epoch 2, Training loss 1.6738504064662376\n",
            "2022-03-30 21:25:51.572340 Epoch 3, Training loss 1.512226495596454\n",
            "2022-03-30 21:26:12.514036 Epoch 4, Training loss 1.4071405790650937\n",
            "2022-03-30 21:26:33.275505 Epoch 5, Training loss 1.328491179991866\n",
            "2022-03-30 21:26:54.151956 Epoch 6, Training loss 1.262372285737406\n",
            "2022-03-30 21:27:15.062363 Epoch 7, Training loss 1.2083584762290311\n",
            "2022-03-30 21:27:35.931698 Epoch 8, Training loss 1.1575619627142806\n",
            "2022-03-30 21:27:56.713547 Epoch 9, Training loss 1.115993541905947\n",
            "2022-03-30 21:28:17.446053 Epoch 10, Training loss 1.0787508140896898\n",
            "2022-03-30 21:28:38.167031 Epoch 11, Training loss 1.0478703122004829\n",
            "2022-03-30 21:28:58.980263 Epoch 12, Training loss 1.0177687699989895\n",
            "2022-03-30 21:29:19.764741 Epoch 13, Training loss 0.9897192240980885\n",
            "2022-03-30 21:29:40.686425 Epoch 14, Training loss 0.967040217212399\n",
            "2022-03-30 21:30:01.404424 Epoch 15, Training loss 0.9461680456348087\n",
            "2022-03-30 21:30:22.239353 Epoch 16, Training loss 0.9249819331156933\n",
            "2022-03-30 21:30:43.161620 Epoch 17, Training loss 0.9094106014579764\n",
            "2022-03-30 21:31:04.002297 Epoch 18, Training loss 0.892863031314767\n",
            "2022-03-30 21:31:24.623884 Epoch 19, Training loss 0.8772607076808315\n",
            "2022-03-30 21:31:45.287368 Epoch 20, Training loss 0.8632810260633679\n",
            "2022-03-30 21:32:05.966429 Epoch 21, Training loss 0.8514301848533513\n",
            "2022-03-30 21:32:26.749162 Epoch 22, Training loss 0.839798701617419\n",
            "2022-03-30 21:32:47.526406 Epoch 23, Training loss 0.8297371209582405\n",
            "2022-03-30 21:33:08.323235 Epoch 24, Training loss 0.819053583895154\n",
            "2022-03-30 21:33:29.134313 Epoch 25, Training loss 0.8094376655262144\n",
            "2022-03-30 21:33:49.915656 Epoch 26, Training loss 0.8006597123944851\n",
            "2022-03-30 21:34:10.737753 Epoch 27, Training loss 0.7934950736477552\n",
            "2022-03-30 21:34:31.510472 Epoch 28, Training loss 0.7835743462719271\n",
            "2022-03-30 21:34:52.313708 Epoch 29, Training loss 0.7754174545597847\n",
            "2022-03-30 21:35:13.013584 Epoch 30, Training loss 0.7676619867915693\n",
            "2022-03-30 21:35:33.736463 Epoch 31, Training loss 0.7625737535526685\n",
            "2022-03-30 21:35:54.559716 Epoch 32, Training loss 0.7549062966919311\n",
            "2022-03-30 21:36:15.308158 Epoch 33, Training loss 0.7486817693085317\n",
            "2022-03-30 21:36:35.952990 Epoch 34, Training loss 0.7412654836388195\n",
            "2022-03-30 21:36:56.580003 Epoch 35, Training loss 0.7345818137878652\n",
            "2022-03-30 21:37:17.208909 Epoch 36, Training loss 0.7279805956822832\n",
            "2022-03-30 21:37:37.827688 Epoch 37, Training loss 0.7232626616345037\n",
            "2022-03-30 21:37:58.400860 Epoch 38, Training loss 0.7184708767077502\n",
            "2022-03-30 21:38:19.052137 Epoch 39, Training loss 0.7128376960754395\n",
            "2022-03-30 21:38:39.633872 Epoch 40, Training loss 0.7071413333763552\n",
            "2022-03-30 21:39:00.262885 Epoch 41, Training loss 0.7021935234594223\n",
            "2022-03-30 21:39:20.782211 Epoch 42, Training loss 0.6972630800645979\n",
            "2022-03-30 21:39:41.240257 Epoch 43, Training loss 0.6918518881282538\n",
            "2022-03-30 21:40:01.748130 Epoch 44, Training loss 0.6886374451925078\n",
            "2022-03-30 21:40:22.339465 Epoch 45, Training loss 0.6841317704876365\n",
            "2022-03-30 21:40:42.943838 Epoch 46, Training loss 0.6780284739211392\n",
            "2022-03-30 21:41:03.656148 Epoch 47, Training loss 0.6729552664643969\n",
            "2022-03-30 21:41:24.230581 Epoch 48, Training loss 0.6692399805997644\n",
            "2022-03-30 21:41:44.812966 Epoch 49, Training loss 0.6662759707330743\n",
            "2022-03-30 21:42:05.364776 Epoch 50, Training loss 0.6646478965383051\n",
            "2022-03-30 21:42:26.007045 Epoch 51, Training loss 0.6560702089534696\n",
            "2022-03-30 21:42:46.552000 Epoch 52, Training loss 0.6544931836597755\n",
            "2022-03-30 21:43:07.160112 Epoch 53, Training loss 0.648468710729838\n",
            "2022-03-30 21:43:27.704560 Epoch 54, Training loss 0.6457766674439925\n",
            "2022-03-30 21:43:48.277038 Epoch 55, Training loss 0.6421829593532226\n",
            "2022-03-30 21:44:08.856868 Epoch 56, Training loss 0.6391603426479012\n",
            "2022-03-30 21:44:29.504591 Epoch 57, Training loss 0.6358617880688909\n",
            "2022-03-30 21:44:50.131334 Epoch 58, Training loss 0.6340581812059788\n",
            "2022-03-30 21:45:10.700882 Epoch 59, Training loss 0.6290968201883004\n",
            "2022-03-30 21:45:31.257891 Epoch 60, Training loss 0.6269663897774104\n",
            "2022-03-30 21:45:51.786716 Epoch 61, Training loss 0.6244186528808321\n",
            "2022-03-30 21:46:12.322684 Epoch 62, Training loss 0.6200767007401532\n",
            "2022-03-30 21:46:32.848592 Epoch 63, Training loss 0.6177006840629651\n",
            "2022-03-30 21:46:53.441820 Epoch 64, Training loss 0.6152040968694346\n",
            "2022-03-30 21:47:13.968759 Epoch 65, Training loss 0.6108831125680748\n",
            "2022-03-30 21:47:34.548470 Epoch 66, Training loss 0.6091565458137361\n",
            "2022-03-30 21:47:54.963357 Epoch 67, Training loss 0.6050063191396197\n",
            "2022-03-30 21:48:15.522383 Epoch 68, Training loss 0.6014984551902927\n",
            "2022-03-30 21:48:36.104363 Epoch 69, Training loss 0.5974104263822136\n",
            "2022-03-30 21:48:56.799478 Epoch 70, Training loss 0.5963134966085634\n",
            "2022-03-30 21:49:17.394761 Epoch 71, Training loss 0.5931881751931841\n",
            "2022-03-30 21:49:37.978107 Epoch 72, Training loss 0.5927331133190629\n",
            "2022-03-30 21:49:58.495069 Epoch 73, Training loss 0.5910746331333809\n",
            "2022-03-30 21:50:19.057328 Epoch 74, Training loss 0.5873106998365248\n",
            "2022-03-30 21:50:39.602009 Epoch 75, Training loss 0.5861606841044658\n",
            "2022-03-30 21:51:00.133691 Epoch 76, Training loss 0.5818221569823487\n",
            "2022-03-30 21:51:20.807059 Epoch 77, Training loss 0.5804924333034573\n",
            "2022-03-30 21:51:41.699244 Epoch 78, Training loss 0.579415930563684\n",
            "2022-03-30 21:52:02.861626 Epoch 79, Training loss 0.5734354466809641\n",
            "2022-03-30 21:52:23.869266 Epoch 80, Training loss 0.5732271141560791\n",
            "2022-03-30 21:52:44.906334 Epoch 81, Training loss 0.5702435206193144\n",
            "2022-03-30 21:53:05.980684 Epoch 82, Training loss 0.5707584810836236\n",
            "2022-03-30 21:53:26.976123 Epoch 83, Training loss 0.5673260708599139\n",
            "2022-03-30 21:53:48.047401 Epoch 84, Training loss 0.5651314228468234\n",
            "2022-03-30 21:54:09.110125 Epoch 85, Training loss 0.5649576615875639\n",
            "2022-03-30 21:54:30.128170 Epoch 86, Training loss 0.5639319886522525\n",
            "2022-03-30 21:54:51.233013 Epoch 87, Training loss 0.5590055047360527\n",
            "2022-03-30 21:55:12.177241 Epoch 88, Training loss 0.5572988808802937\n",
            "2022-03-30 21:55:33.238031 Epoch 89, Training loss 0.5537049433078303\n",
            "2022-03-30 21:55:54.329863 Epoch 90, Training loss 0.5547347065356686\n",
            "2022-03-30 21:56:15.353714 Epoch 91, Training loss 0.551967875388882\n",
            "2022-03-30 21:56:36.385308 Epoch 92, Training loss 0.5506616553763295\n",
            "2022-03-30 21:56:57.638087 Epoch 93, Training loss 0.549320496752134\n",
            "2022-03-30 21:57:18.731578 Epoch 94, Training loss 0.5454721369821093\n",
            "2022-03-30 21:57:39.673838 Epoch 95, Training loss 0.5427766813875159\n",
            "2022-03-30 21:58:00.816888 Epoch 96, Training loss 0.5416826831029199\n",
            "2022-03-30 21:58:21.934189 Epoch 97, Training loss 0.5385557559635633\n",
            "2022-03-30 21:58:42.786013 Epoch 98, Training loss 0.5391241964857901\n",
            "2022-03-30 21:59:03.839826 Epoch 99, Training loss 0.5382676829233803\n",
            "2022-03-30 21:59:24.881581 Epoch 100, Training loss 0.5376552665020193\n",
            "2022-03-30 21:59:45.700140 Epoch 101, Training loss 0.5325537004586681\n",
            "2022-03-30 22:00:06.597724 Epoch 102, Training loss 0.532721923764252\n",
            "2022-03-30 22:00:27.670226 Epoch 103, Training loss 0.5304228067779175\n",
            "2022-03-30 22:00:48.710448 Epoch 104, Training loss 0.5277588798871735\n",
            "2022-03-30 22:01:09.848409 Epoch 105, Training loss 0.5256526787643847\n",
            "2022-03-30 22:01:30.835529 Epoch 106, Training loss 0.5258363953712956\n",
            "2022-03-30 22:01:51.921744 Epoch 107, Training loss 0.5236427862473461\n",
            "2022-03-30 22:02:12.875974 Epoch 108, Training loss 0.5213200265298719\n",
            "2022-03-30 22:02:33.850681 Epoch 109, Training loss 0.5209990714288428\n",
            "2022-03-30 22:02:54.804318 Epoch 110, Training loss 0.518895854501773\n",
            "2022-03-30 22:03:15.821916 Epoch 111, Training loss 0.519344268895476\n",
            "2022-03-30 22:03:36.789728 Epoch 112, Training loss 0.5163359134017355\n",
            "2022-03-30 22:03:57.866708 Epoch 113, Training loss 0.5129322934028743\n",
            "2022-03-30 22:04:18.906085 Epoch 114, Training loss 0.5112030991850911\n",
            "2022-03-30 22:04:40.212055 Epoch 115, Training loss 0.5096596840321256\n",
            "2022-03-30 22:05:01.158692 Epoch 116, Training loss 0.5114888873170403\n",
            "2022-03-30 22:05:22.143535 Epoch 117, Training loss 0.5118176230917806\n",
            "2022-03-30 22:05:43.205088 Epoch 118, Training loss 0.5075716039027705\n",
            "2022-03-30 22:06:04.267614 Epoch 119, Training loss 0.5067837520519181\n",
            "2022-03-30 22:06:25.341851 Epoch 120, Training loss 0.5059977994893518\n",
            "2022-03-30 22:06:46.302352 Epoch 121, Training loss 0.504961086394232\n",
            "2022-03-30 22:07:07.293932 Epoch 122, Training loss 0.501801850964956\n",
            "2022-03-30 22:07:28.216546 Epoch 123, Training loss 0.5010135554901475\n",
            "2022-03-30 22:07:49.270373 Epoch 124, Training loss 0.5000542225816366\n",
            "2022-03-30 22:08:10.306578 Epoch 125, Training loss 0.49809880272659196\n",
            "2022-03-30 22:08:31.243852 Epoch 126, Training loss 0.49768150940804223\n",
            "2022-03-30 22:08:52.393777 Epoch 127, Training loss 0.4962918782211326\n",
            "2022-03-30 22:09:13.269370 Epoch 128, Training loss 0.4953658616032137\n",
            "2022-03-30 22:09:34.165601 Epoch 129, Training loss 0.49241289612658495\n",
            "2022-03-30 22:09:55.034224 Epoch 130, Training loss 0.4913404682065215\n",
            "2022-03-30 22:10:16.037627 Epoch 131, Training loss 0.48986514484333565\n",
            "2022-03-30 22:10:37.062545 Epoch 132, Training loss 0.4903845105444074\n",
            "2022-03-30 22:10:57.894583 Epoch 133, Training loss 0.4877746582907789\n",
            "2022-03-30 22:11:18.832623 Epoch 134, Training loss 0.4875256523032627\n",
            "2022-03-30 22:11:39.827589 Epoch 135, Training loss 0.48564752701984343\n",
            "2022-03-30 22:12:00.995372 Epoch 136, Training loss 0.48517377068624473\n",
            "2022-03-30 22:12:22.055863 Epoch 137, Training loss 0.48575824379082533\n",
            "2022-03-30 22:12:43.108021 Epoch 138, Training loss 0.480229546575595\n",
            "2022-03-30 22:13:03.947236 Epoch 139, Training loss 0.4817630670526448\n",
            "2022-03-30 22:13:24.873439 Epoch 140, Training loss 0.47817888573917283\n",
            "2022-03-30 22:13:45.812363 Epoch 141, Training loss 0.47744639936234334\n",
            "2022-03-30 22:14:06.723785 Epoch 142, Training loss 0.47823153882075453\n",
            "2022-03-30 22:14:27.755307 Epoch 143, Training loss 0.478910920203037\n",
            "2022-03-30 22:14:48.728205 Epoch 144, Training loss 0.47701570054378045\n",
            "2022-03-30 22:15:09.568614 Epoch 145, Training loss 0.4739497386471695\n",
            "2022-03-30 22:15:30.458456 Epoch 146, Training loss 0.4722839586646356\n",
            "2022-03-30 22:15:51.327459 Epoch 147, Training loss 0.47244876444034867\n",
            "2022-03-30 22:16:12.073614 Epoch 148, Training loss 0.47307037548793246\n",
            "2022-03-30 22:16:32.810372 Epoch 149, Training loss 0.4692914739365468\n",
            "2022-03-30 22:16:53.537201 Epoch 150, Training loss 0.4700498518812687\n",
            "2022-03-30 22:17:14.436687 Epoch 151, Training loss 0.46915575642796126\n",
            "2022-03-30 22:17:35.213850 Epoch 152, Training loss 0.46933356563910805\n",
            "2022-03-30 22:17:55.893968 Epoch 153, Training loss 0.4702986310357633\n",
            "2022-03-30 22:18:16.603029 Epoch 154, Training loss 0.46723648124491163\n",
            "2022-03-30 22:18:37.332605 Epoch 155, Training loss 0.46488469328416887\n",
            "2022-03-30 22:18:58.114546 Epoch 156, Training loss 0.4643951886145355\n",
            "2022-03-30 22:19:18.984038 Epoch 157, Training loss 0.4626445980823558\n",
            "2022-03-30 22:19:39.848114 Epoch 158, Training loss 0.46361177679522875\n",
            "2022-03-30 22:20:00.480275 Epoch 159, Training loss 0.463786375072911\n",
            "2022-03-30 22:20:21.262164 Epoch 160, Training loss 0.46113851851285875\n",
            "2022-03-30 22:20:42.180652 Epoch 161, Training loss 0.4613014228279938\n",
            "2022-03-30 22:21:03.237878 Epoch 162, Training loss 0.4587339406542461\n",
            "2022-03-30 22:21:24.221243 Epoch 163, Training loss 0.4596178232480193\n",
            "2022-03-30 22:21:45.158432 Epoch 164, Training loss 0.460561321633856\n",
            "2022-03-30 22:22:06.102269 Epoch 165, Training loss 0.45669438985302624\n",
            "2022-03-30 22:22:27.224767 Epoch 166, Training loss 0.4541511450467817\n",
            "2022-03-30 22:22:48.294651 Epoch 167, Training loss 0.4550235575765295\n",
            "2022-03-30 22:23:09.479987 Epoch 168, Training loss 0.4538793940563946\n",
            "2022-03-30 22:23:30.475573 Epoch 169, Training loss 0.45370219139110707\n",
            "2022-03-30 22:23:51.438390 Epoch 170, Training loss 0.4538585742378174\n",
            "2022-03-30 22:24:12.333772 Epoch 171, Training loss 0.45153526675975536\n",
            "2022-03-30 22:24:33.180859 Epoch 172, Training loss 0.4514702553324916\n",
            "2022-03-30 22:24:54.197504 Epoch 173, Training loss 0.44935952226066833\n",
            "2022-03-30 22:25:15.105316 Epoch 174, Training loss 0.45006027594780373\n",
            "2022-03-30 22:25:35.947473 Epoch 175, Training loss 0.44903092002472306\n",
            "2022-03-30 22:25:56.823359 Epoch 176, Training loss 0.4487109693801007\n",
            "2022-03-30 22:26:17.651553 Epoch 177, Training loss 0.4484155028105697\n",
            "2022-03-30 22:26:38.506210 Epoch 178, Training loss 0.4457134076529909\n",
            "2022-03-30 22:26:59.324521 Epoch 179, Training loss 0.44651846772493303\n",
            "2022-03-30 22:27:20.201421 Epoch 180, Training loss 0.4422271114290523\n",
            "2022-03-30 22:27:41.058438 Epoch 181, Training loss 0.4456890178344134\n",
            "2022-03-30 22:28:02.069570 Epoch 182, Training loss 0.4451329222382487\n",
            "2022-03-30 22:28:23.045948 Epoch 183, Training loss 0.4415433649783549\n",
            "2022-03-30 22:28:44.092085 Epoch 184, Training loss 0.4427179595088715\n",
            "2022-03-30 22:29:05.049910 Epoch 185, Training loss 0.44157417352928224\n",
            "2022-03-30 22:29:25.888682 Epoch 186, Training loss 0.44011498747579275\n",
            "2022-03-30 22:29:46.902244 Epoch 187, Training loss 0.43942402901551914\n",
            "2022-03-30 22:30:07.899768 Epoch 188, Training loss 0.4379863202038323\n",
            "2022-03-30 22:30:28.919508 Epoch 189, Training loss 0.43909172130667645\n",
            "2022-03-30 22:30:49.841709 Epoch 190, Training loss 0.4386663191839862\n",
            "2022-03-30 22:31:10.764043 Epoch 191, Training loss 0.4365849008264444\n",
            "2022-03-30 22:31:31.620119 Epoch 192, Training loss 0.43794686250064685\n",
            "2022-03-30 22:31:52.585030 Epoch 193, Training loss 0.4338571987379238\n",
            "2022-03-30 22:32:13.545854 Epoch 194, Training loss 0.4355407665910013\n",
            "2022-03-30 22:32:34.529503 Epoch 195, Training loss 0.4355535607051362\n",
            "2022-03-30 22:32:55.570995 Epoch 196, Training loss 0.43362664942965484\n",
            "2022-03-30 22:33:16.585666 Epoch 197, Training loss 0.43181437093888403\n",
            "2022-03-30 22:33:37.491436 Epoch 198, Training loss 0.4340068418008592\n",
            "2022-03-30 22:33:58.415875 Epoch 199, Training loss 0.43498278447353017\n",
            "2022-03-30 22:34:19.283847 Epoch 200, Training loss 0.43135480677990046\n",
            "2022-03-30 22:34:40.217377 Epoch 201, Training loss 0.4302582204761103\n",
            "2022-03-30 22:35:01.155459 Epoch 202, Training loss 0.4303363548291614\n",
            "2022-03-30 22:35:22.052501 Epoch 203, Training loss 0.4280351325679008\n",
            "2022-03-30 22:35:43.067329 Epoch 204, Training loss 0.4276141434183816\n",
            "2022-03-30 22:36:03.967237 Epoch 205, Training loss 0.42574758661906126\n",
            "2022-03-30 22:36:24.915566 Epoch 206, Training loss 0.42932249771435854\n",
            "2022-03-30 22:36:45.797363 Epoch 207, Training loss 0.4258147589195415\n",
            "2022-03-30 22:37:06.673334 Epoch 208, Training loss 0.42559277098578263\n",
            "2022-03-30 22:37:27.634362 Epoch 209, Training loss 0.4252303381976874\n",
            "2022-03-30 22:37:48.716541 Epoch 210, Training loss 0.42820847781417926\n",
            "2022-03-30 22:38:09.763615 Epoch 211, Training loss 0.42276294653296775\n",
            "2022-03-30 22:38:30.702831 Epoch 212, Training loss 0.42611133644495475\n",
            "2022-03-30 22:38:51.673747 Epoch 213, Training loss 0.42456048534578067\n",
            "2022-03-30 22:39:12.741768 Epoch 214, Training loss 0.42351114744191887\n",
            "2022-03-30 22:39:33.806590 Epoch 215, Training loss 0.42302370237191317\n",
            "2022-03-30 22:39:54.756008 Epoch 216, Training loss 0.41850204207479497\n",
            "2022-03-30 22:40:15.747363 Epoch 217, Training loss 0.4234730993466609\n",
            "2022-03-30 22:40:36.746688 Epoch 218, Training loss 0.42075347353506576\n",
            "2022-03-30 22:40:57.763735 Epoch 219, Training loss 0.4206465577034999\n",
            "2022-03-30 22:41:18.628986 Epoch 220, Training loss 0.4187804489489407\n",
            "2022-03-30 22:41:39.478042 Epoch 221, Training loss 0.41880779953487696\n",
            "2022-03-30 22:42:00.474490 Epoch 222, Training loss 0.41781303429466377\n",
            "2022-03-30 22:42:21.511855 Epoch 223, Training loss 0.4192198323052558\n",
            "2022-03-30 22:42:42.468137 Epoch 224, Training loss 0.41507520701955347\n",
            "2022-03-30 22:43:03.459997 Epoch 225, Training loss 0.41463324320895595\n",
            "2022-03-30 22:43:24.445660 Epoch 226, Training loss 0.4151679862986135\n",
            "2022-03-30 22:43:45.345043 Epoch 227, Training loss 0.4139124363508371\n",
            "2022-03-30 22:44:06.297911 Epoch 228, Training loss 0.4125315981114383\n",
            "2022-03-30 22:44:27.186628 Epoch 229, Training loss 0.4162907977314556\n",
            "2022-03-30 22:44:48.007789 Epoch 230, Training loss 0.41224488050050445\n",
            "2022-03-30 22:45:08.916883 Epoch 231, Training loss 0.4125948415311706\n",
            "2022-03-30 22:45:29.838730 Epoch 232, Training loss 0.410998290342748\n",
            "2022-03-30 22:45:50.744188 Epoch 233, Training loss 0.4107261133354033\n",
            "2022-03-30 22:46:11.696944 Epoch 234, Training loss 0.4103557008230473\n",
            "2022-03-30 22:46:32.619356 Epoch 235, Training loss 0.4124129643220731\n",
            "2022-03-30 22:46:53.681253 Epoch 236, Training loss 0.4057404101466584\n",
            "2022-03-30 22:47:14.687680 Epoch 237, Training loss 0.4085215239992837\n",
            "2022-03-30 22:47:35.450101 Epoch 238, Training loss 0.4112740516319604\n",
            "2022-03-30 22:47:56.504476 Epoch 239, Training loss 0.4055124310505055\n",
            "2022-03-30 22:48:17.679571 Epoch 240, Training loss 0.40914913274519277\n",
            "2022-03-30 22:48:38.674426 Epoch 241, Training loss 0.4061159990403963\n",
            "2022-03-30 22:48:59.614339 Epoch 242, Training loss 0.40863692221205555\n",
            "2022-03-30 22:49:20.596840 Epoch 243, Training loss 0.4064057190971606\n",
            "2022-03-30 22:49:41.569990 Epoch 244, Training loss 0.4063028303520454\n",
            "2022-03-30 22:50:02.374748 Epoch 245, Training loss 0.4066428403796442\n",
            "2022-03-30 22:50:23.175669 Epoch 246, Training loss 0.40466244863655865\n",
            "2022-03-30 22:50:44.074804 Epoch 247, Training loss 0.40532241395824703\n",
            "2022-03-30 22:51:05.071379 Epoch 248, Training loss 0.4062324476897564\n",
            "2022-03-30 22:51:26.138003 Epoch 249, Training loss 0.3994961699180286\n",
            "2022-03-30 22:51:47.092583 Epoch 250, Training loss 0.4036450656059453\n",
            "2022-03-30 22:52:08.017372 Epoch 251, Training loss 0.4033135455911574\n",
            "2022-03-30 22:52:28.907701 Epoch 252, Training loss 0.40267457254707356\n",
            "2022-03-30 22:52:49.964435 Epoch 253, Training loss 0.4000888240459325\n",
            "2022-03-30 22:53:10.961162 Epoch 254, Training loss 0.40233396765444895\n",
            "2022-03-30 22:53:31.937800 Epoch 255, Training loss 0.40445481975327063\n",
            "2022-03-30 22:53:53.045942 Epoch 256, Training loss 0.4013790611720756\n",
            "2022-03-30 22:54:14.183098 Epoch 257, Training loss 0.3996293087825751\n",
            "2022-03-30 22:54:35.179457 Epoch 258, Training loss 0.3988503210837274\n",
            "2022-03-30 22:54:56.296661 Epoch 259, Training loss 0.4011509467268844\n",
            "2022-03-30 22:55:17.372406 Epoch 260, Training loss 0.3984284871412665\n",
            "2022-03-30 22:55:38.586934 Epoch 261, Training loss 0.3977905162955489\n",
            "2022-03-30 22:55:59.695800 Epoch 262, Training loss 0.39692160839696067\n",
            "2022-03-30 22:56:20.719992 Epoch 263, Training loss 0.3943095299822595\n",
            "2022-03-30 22:56:41.720441 Epoch 264, Training loss 0.3974941046074833\n",
            "2022-03-30 22:57:02.688236 Epoch 265, Training loss 0.39551851214350336\n",
            "2022-03-30 22:57:23.743691 Epoch 266, Training loss 0.3959539622792502\n",
            "2022-03-30 22:57:44.798288 Epoch 267, Training loss 0.3929489429878152\n",
            "2022-03-30 22:58:05.841013 Epoch 268, Training loss 0.3939706587692356\n",
            "2022-03-30 22:58:26.901110 Epoch 269, Training loss 0.39475249929729933\n",
            "2022-03-30 22:58:48.004774 Epoch 270, Training loss 0.3959615733807959\n",
            "2022-03-30 22:59:08.943477 Epoch 271, Training loss 0.39338804971988856\n",
            "2022-03-30 22:59:29.947441 Epoch 272, Training loss 0.39581534296960175\n",
            "2022-03-30 22:59:51.006060 Epoch 273, Training loss 0.3900569305586083\n",
            "2022-03-30 23:00:12.088730 Epoch 274, Training loss 0.39445656364607384\n",
            "2022-03-30 23:00:33.059924 Epoch 275, Training loss 0.392334727984865\n",
            "2022-03-30 23:00:54.065230 Epoch 276, Training loss 0.3915083627680988\n",
            "2022-03-30 23:01:15.143880 Epoch 277, Training loss 0.3904181789997441\n",
            "2022-03-30 23:01:36.184321 Epoch 278, Training loss 0.39259858235068945\n",
            "2022-03-30 23:01:57.243928 Epoch 279, Training loss 0.38897337373870106\n",
            "2022-03-30 23:02:18.274936 Epoch 280, Training loss 0.3904731192285448\n",
            "2022-03-30 23:02:39.263230 Epoch 281, Training loss 0.38779608110713837\n",
            "2022-03-30 23:03:00.244695 Epoch 282, Training loss 0.3897618512096612\n",
            "2022-03-30 23:03:21.315347 Epoch 283, Training loss 0.3879724259076216\n",
            "2022-03-30 23:03:42.350772 Epoch 284, Training loss 0.3889036375810118\n",
            "2022-03-30 23:04:03.284953 Epoch 285, Training loss 0.38789158792751827\n",
            "2022-03-30 23:04:24.150355 Epoch 286, Training loss 0.3870028618275357\n",
            "2022-03-30 23:04:45.079052 Epoch 287, Training loss 0.3864436024785652\n",
            "2022-03-30 23:05:06.083670 Epoch 288, Training loss 0.3832337606098036\n",
            "2022-03-30 23:05:26.958755 Epoch 289, Training loss 0.3854726063244788\n",
            "2022-03-30 23:05:47.858413 Epoch 290, Training loss 0.38903129432359923\n",
            "2022-03-30 23:06:08.824804 Epoch 291, Training loss 0.3860023925104714\n",
            "2022-03-30 23:06:29.698750 Epoch 292, Training loss 0.38582978703443654\n",
            "2022-03-30 23:06:50.592477 Epoch 293, Training loss 0.38132219944539886\n",
            "2022-03-30 23:07:11.630482 Epoch 294, Training loss 0.3822663189066798\n",
            "2022-03-30 23:07:32.615832 Epoch 295, Training loss 0.3852832624426736\n",
            "2022-03-30 23:07:53.585228 Epoch 296, Training loss 0.3845864445390299\n",
            "2022-03-30 23:08:14.669546 Epoch 297, Training loss 0.3779727069237043\n",
            "2022-03-30 23:08:35.719668 Epoch 298, Training loss 0.38016201010750383\n",
            "2022-03-30 23:08:56.617817 Epoch 299, Training loss 0.3832846736473501\n",
            "2022-03-30 23:09:17.531066 Epoch 300, Training loss 0.3795843960912636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6omBhsIZKSEK",
        "outputId": "bcd3ce50-6dc6-4ac2-f306-058573c1e808"
      },
      "id": "6omBhsIZKSEK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.87\n",
            "Accuracy val: 0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## Part B ############"
      ],
      "metadata": {
        "id": "V5kTh9b6Ha5a"
      },
      "id": "V5kTh9b6Ha5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "      loss_train = 0.0\n",
        "      for imgs, labels in train_loader:\n",
        "          imgs = imgs.to(device=device)\n",
        "          labels = labels.to(device=device)\n",
        "          outputs = model(imgs)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          l2_lambda = 0.001\n",
        "          l2_norm = sum(p.pow(2.0).sum()\n",
        "                  for p in model.parameters())\n",
        "          loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss_train += loss.item()\n",
        "      if epoch == 1 or epoch % 10 == 0:\n",
        "          print('{} Epoch {}, Training loss {}'.format(\n",
        "              datetime.datetime.now(), epoch,\n",
        "              loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "WSbKbO3RFhL_"
      },
      "id": "WSbKbO3RFhL_",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop_l2reg(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "0xkubPlPGXJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcca14f-4d06-487d-ec62-3147bb698f48"
      },
      "id": "0xkubPlPGXJ6",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 23:21:47.844961 Epoch 1, Training loss 2.142087420539173\n",
            "2022-03-30 23:23:29.516536 Epoch 10, Training loss 1.1538939296894366\n",
            "2022-03-30 23:25:22.676706 Epoch 20, Training loss 0.9667551664592665\n",
            "2022-03-30 23:27:14.761352 Epoch 30, Training loss 0.8843781549454955\n",
            "2022-03-30 23:29:06.870703 Epoch 40, Training loss 0.837581584025222\n",
            "2022-03-30 23:30:58.987060 Epoch 50, Training loss 0.80340772989156\n",
            "2022-03-30 23:32:50.664595 Epoch 60, Training loss 0.7811093477489394\n",
            "2022-03-30 23:34:42.870576 Epoch 70, Training loss 0.7634496121760219\n",
            "2022-03-30 23:36:34.860892 Epoch 80, Training loss 0.7529852288534574\n",
            "2022-03-30 23:38:26.240298 Epoch 90, Training loss 0.7438377430829246\n",
            "2022-03-30 23:40:17.776408 Epoch 100, Training loss 0.7361932923193173\n",
            "2022-03-30 23:42:09.253360 Epoch 110, Training loss 0.7312463601227002\n",
            "2022-03-30 23:44:00.375576 Epoch 120, Training loss 0.7255655428027863\n",
            "2022-03-30 23:45:51.811823 Epoch 130, Training loss 0.7176811651271933\n",
            "2022-03-30 23:47:43.245323 Epoch 140, Training loss 0.7167427375188569\n",
            "2022-03-30 23:49:34.422381 Epoch 150, Training loss 0.7105309516191483\n",
            "2022-03-30 23:51:25.771111 Epoch 160, Training loss 0.7080988720478609\n",
            "2022-03-30 23:53:16.822012 Epoch 170, Training loss 0.7049970703814036\n",
            "2022-03-30 23:55:08.276595 Epoch 180, Training loss 0.7044999078868905\n",
            "2022-03-30 23:56:59.748340 Epoch 190, Training loss 0.7020587889129853\n",
            "2022-03-30 23:58:51.290382 Epoch 200, Training loss 0.7003710782131576\n",
            "2022-03-31 00:00:42.720806 Epoch 210, Training loss 0.6999026178703893\n",
            "2022-03-31 00:02:34.107391 Epoch 220, Training loss 0.6986693749799753\n",
            "2022-03-31 00:04:25.249086 Epoch 230, Training loss 0.6970268938776172\n",
            "2022-03-31 00:06:16.655422 Epoch 240, Training loss 0.6949813321347127\n",
            "2022-03-31 00:08:07.811373 Epoch 250, Training loss 0.6915967464447021\n",
            "2022-03-31 00:09:59.475345 Epoch 260, Training loss 0.6926396970858659\n",
            "2022-03-31 00:11:51.207571 Epoch 270, Training loss 0.6916005056913551\n",
            "2022-03-31 00:13:43.149031 Epoch 280, Training loss 0.6928985046075128\n",
            "2022-03-31 00:15:34.738669 Epoch 290, Training loss 0.6917987174314001\n",
            "2022-03-31 00:17:29.194191 Epoch 300, Training loss 0.68724981095175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGxgPC1gUMk0",
        "outputId": "caf6dd7d-1a6a-46d5-cbff-f1a1fce693fa"
      },
      "id": "gGxgPC1gUMk0",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.66\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetDropout(nn.Module):\n",
        "  def __init__(self, n_chans1=32):\n",
        "      super().__init__()\n",
        "      self.n_chans1 = n_chans1\n",
        "      self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "      self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
        "      self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "      self.conv2_dropout = nn.Dropout2d(p=0.3)\n",
        "      self.fc1 = nn.Linear(8*8*n_chans1 // 2, 32)\n",
        "      self.fc2 = nn.Linear(32,2)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "      out = self.conv1_dropout(out)\n",
        "      out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "      out = self.conv2_dropout(out)\n",
        "      out = out.view(-1, 8*8*self.n_chans1 // 2)\n",
        "      out = torch.tanh(self.fc1(out))\n",
        "      out = self.fc2(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "_X-TgETsU6sU"
      },
      "id": "_X-TgETsU6sU",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRqAcV1VVJ40",
        "outputId": "fe1fffac-67f2-4705-9dbd-ded5e960ceae"
      },
      "id": "SRqAcV1VVJ40",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-31 00:22:18.322547 Epoch 1, Training loss 2.0831853264128157\n",
            "2022-03-31 00:22:29.251249 Epoch 2, Training loss 1.666690823672068\n",
            "2022-03-31 00:22:40.198290 Epoch 3, Training loss 1.4858769315587894\n",
            "2022-03-31 00:22:51.039498 Epoch 4, Training loss 1.3841847990022595\n",
            "2022-03-31 00:23:02.083261 Epoch 5, Training loss 1.3068497464479998\n",
            "2022-03-31 00:23:13.026683 Epoch 6, Training loss 1.2475846812243352\n",
            "2022-03-31 00:23:23.868023 Epoch 7, Training loss 1.1911033019232933\n",
            "2022-03-31 00:23:34.770645 Epoch 8, Training loss 1.140045927461151\n",
            "2022-03-31 00:23:45.639474 Epoch 9, Training loss 1.0972352738270674\n",
            "2022-03-31 00:23:56.486347 Epoch 10, Training loss 1.060932628562688\n",
            "2022-03-31 00:24:07.450286 Epoch 11, Training loss 1.0286064064868576\n",
            "2022-03-31 00:24:18.313123 Epoch 12, Training loss 1.0009670899347272\n",
            "2022-03-31 00:24:29.295649 Epoch 13, Training loss 0.9783376996474498\n",
            "2022-03-31 00:24:40.274626 Epoch 14, Training loss 0.9572183841939472\n",
            "2022-03-31 00:24:51.038089 Epoch 15, Training loss 0.9387706160697791\n",
            "2022-03-31 00:25:01.914398 Epoch 16, Training loss 0.9188331759646725\n",
            "2022-03-31 00:25:12.879525 Epoch 17, Training loss 0.9053562264460737\n",
            "2022-03-31 00:25:23.852036 Epoch 18, Training loss 0.8893500243306465\n",
            "2022-03-31 00:25:34.732971 Epoch 19, Training loss 0.8744968811569311\n",
            "2022-03-31 00:25:45.626746 Epoch 20, Training loss 0.8624068649147477\n",
            "2022-03-31 00:25:56.456953 Epoch 21, Training loss 0.850454924051719\n",
            "2022-03-31 00:26:07.382300 Epoch 22, Training loss 0.8404920492559442\n",
            "2022-03-31 00:26:18.341077 Epoch 23, Training loss 0.8304804174796395\n",
            "2022-03-31 00:26:29.285443 Epoch 24, Training loss 0.8194605286621377\n",
            "2022-03-31 00:26:40.117643 Epoch 25, Training loss 0.8118631244086854\n",
            "2022-03-31 00:26:50.970440 Epoch 26, Training loss 0.8016359314817907\n",
            "2022-03-31 00:27:01.701599 Epoch 27, Training loss 0.793817472884722\n",
            "2022-03-31 00:27:12.484326 Epoch 28, Training loss 0.7862840568089424\n",
            "2022-03-31 00:27:23.453550 Epoch 29, Training loss 0.7782226731557675\n",
            "2022-03-31 00:27:34.406229 Epoch 30, Training loss 0.770669283693099\n",
            "2022-03-31 00:27:45.128231 Epoch 31, Training loss 0.7626134622889711\n",
            "2022-03-31 00:27:55.851264 Epoch 32, Training loss 0.7564675365491291\n",
            "2022-03-31 00:28:06.551166 Epoch 33, Training loss 0.7513559158805692\n",
            "2022-03-31 00:28:17.405305 Epoch 34, Training loss 0.7436877961277657\n",
            "2022-03-31 00:28:28.170367 Epoch 35, Training loss 0.7385923594541257\n",
            "2022-03-31 00:28:38.890232 Epoch 36, Training loss 0.7308966107761768\n",
            "2022-03-31 00:28:49.578353 Epoch 37, Training loss 0.7283693645387659\n",
            "2022-03-31 00:29:00.386852 Epoch 38, Training loss 0.7208633113395223\n",
            "2022-03-31 00:29:11.211960 Epoch 39, Training loss 0.7181817400638405\n",
            "2022-03-31 00:29:22.038103 Epoch 40, Training loss 0.7116181131671456\n",
            "2022-03-31 00:29:32.809060 Epoch 41, Training loss 0.7067595046499501\n",
            "2022-03-31 00:29:43.539452 Epoch 42, Training loss 0.7008053224791041\n",
            "2022-03-31 00:29:54.283737 Epoch 43, Training loss 0.6965185580274943\n",
            "2022-03-31 00:30:04.742269 Epoch 44, Training loss 0.6929716870303044\n",
            "2022-03-31 00:30:15.219520 Epoch 45, Training loss 0.6885760929578405\n",
            "2022-03-31 00:30:25.772858 Epoch 46, Training loss 0.6841433814267064\n",
            "2022-03-31 00:30:36.204243 Epoch 47, Training loss 0.6784061845534902\n",
            "2022-03-31 00:30:46.737433 Epoch 48, Training loss 0.6754844568650741\n",
            "2022-03-31 00:30:57.304091 Epoch 49, Training loss 0.6717610226567748\n",
            "2022-03-31 00:31:07.845458 Epoch 50, Training loss 0.666265452152018\n",
            "2022-03-31 00:31:18.558110 Epoch 51, Training loss 0.6646323720055163\n",
            "2022-03-31 00:31:29.222397 Epoch 52, Training loss 0.6587468209245321\n",
            "2022-03-31 00:31:39.757428 Epoch 53, Training loss 0.6556509612390148\n",
            "2022-03-31 00:31:50.235835 Epoch 54, Training loss 0.6523312747554706\n",
            "2022-03-31 00:32:00.868975 Epoch 55, Training loss 0.6466716910567125\n",
            "2022-03-31 00:32:11.320131 Epoch 56, Training loss 0.6451657584408665\n",
            "2022-03-31 00:32:21.947686 Epoch 57, Training loss 0.6429979146441536\n",
            "2022-03-31 00:32:32.567836 Epoch 58, Training loss 0.6387550218407151\n",
            "2022-03-31 00:32:43.113124 Epoch 59, Training loss 0.6357357810296671\n",
            "2022-03-31 00:32:53.579451 Epoch 60, Training loss 0.6330456317919294\n",
            "2022-03-31 00:33:04.251011 Epoch 61, Training loss 0.6293006361369282\n",
            "2022-03-31 00:33:14.763217 Epoch 62, Training loss 0.62829630252193\n",
            "2022-03-31 00:33:25.370787 Epoch 63, Training loss 0.6240450061495652\n",
            "2022-03-31 00:33:35.938578 Epoch 64, Training loss 0.6203585007535223\n",
            "2022-03-31 00:33:46.494656 Epoch 65, Training loss 0.6192268560761991\n",
            "2022-03-31 00:33:56.988257 Epoch 66, Training loss 0.6155777560246875\n",
            "2022-03-31 00:34:07.563062 Epoch 67, Training loss 0.609936451591799\n",
            "2022-03-31 00:34:18.011930 Epoch 68, Training loss 0.6096479481900744\n",
            "2022-03-31 00:34:28.599838 Epoch 69, Training loss 0.6077466932937617\n",
            "2022-03-31 00:34:39.168472 Epoch 70, Training loss 0.6055814160029297\n",
            "2022-03-31 00:34:49.781896 Epoch 71, Training loss 0.602235415631243\n",
            "2022-03-31 00:35:00.325367 Epoch 72, Training loss 0.6011276400226462\n",
            "2022-03-31 00:35:11.073331 Epoch 73, Training loss 0.5965206848119226\n",
            "2022-03-31 00:35:21.666740 Epoch 74, Training loss 0.5949678498765697\n",
            "2022-03-31 00:35:32.201660 Epoch 75, Training loss 0.5905484807537035\n",
            "2022-03-31 00:35:42.799373 Epoch 76, Training loss 0.5902166874969706\n",
            "2022-03-31 00:35:53.350038 Epoch 77, Training loss 0.5877034715984178\n",
            "2022-03-31 00:36:03.840355 Epoch 78, Training loss 0.5867491886972467\n",
            "2022-03-31 00:36:14.274250 Epoch 79, Training loss 0.5813973600144886\n",
            "2022-03-31 00:36:24.847011 Epoch 80, Training loss 0.582256415120476\n",
            "2022-03-31 00:36:35.484238 Epoch 81, Training loss 0.578584903036542\n",
            "2022-03-31 00:36:45.984690 Epoch 82, Training loss 0.5761000773181086\n",
            "2022-03-31 00:36:56.523443 Epoch 83, Training loss 0.5749452141544703\n",
            "2022-03-31 00:37:07.244004 Epoch 84, Training loss 0.5731245503401208\n",
            "2022-03-31 00:37:17.985204 Epoch 85, Training loss 0.571217019158556\n",
            "2022-03-31 00:37:28.673854 Epoch 86, Training loss 0.5705187941908532\n",
            "2022-03-31 00:37:39.306460 Epoch 87, Training loss 0.5661385751060207\n",
            "2022-03-31 00:37:49.729708 Epoch 88, Training loss 0.5636910605232429\n",
            "2022-03-31 00:38:00.299109 Epoch 89, Training loss 0.561897726543724\n",
            "2022-03-31 00:38:10.794316 Epoch 90, Training loss 0.5626862693739973\n",
            "2022-03-31 00:38:21.349678 Epoch 91, Training loss 0.556272149619544\n",
            "2022-03-31 00:38:31.788439 Epoch 92, Training loss 0.5535510783381474\n",
            "2022-03-31 00:38:42.407430 Epoch 93, Training loss 0.5534597657754293\n",
            "2022-03-31 00:38:53.026099 Epoch 94, Training loss 0.5546173783748046\n",
            "2022-03-31 00:39:03.602644 Epoch 95, Training loss 0.5512282280894496\n",
            "2022-03-31 00:39:14.148544 Epoch 96, Training loss 0.5505491869376443\n",
            "2022-03-31 00:39:24.555514 Epoch 97, Training loss 0.5478721543231888\n",
            "2022-03-31 00:39:35.136752 Epoch 98, Training loss 0.5463314969140245\n",
            "2022-03-31 00:39:45.733391 Epoch 99, Training loss 0.5442729971712202\n",
            "2022-03-31 00:39:56.278042 Epoch 100, Training loss 0.5422265450744068\n",
            "2022-03-31 00:40:06.756489 Epoch 101, Training loss 0.5408361616646847\n",
            "2022-03-31 00:40:17.177859 Epoch 102, Training loss 0.5387162480817731\n",
            "2022-03-31 00:40:27.651906 Epoch 103, Training loss 0.5393241425533124\n",
            "2022-03-31 00:40:38.093752 Epoch 104, Training loss 0.5359279988976695\n",
            "2022-03-31 00:40:48.590809 Epoch 105, Training loss 0.5340209619887649\n",
            "2022-03-31 00:40:59.040006 Epoch 106, Training loss 0.5324028336712162\n",
            "2022-03-31 00:41:09.596857 Epoch 107, Training loss 0.5321107922726885\n",
            "2022-03-31 00:41:20.083349 Epoch 108, Training loss 0.5290543322291825\n",
            "2022-03-31 00:41:30.555425 Epoch 109, Training loss 0.52878476719341\n",
            "2022-03-31 00:41:41.001656 Epoch 110, Training loss 0.5267226351496509\n",
            "2022-03-31 00:41:51.477378 Epoch 111, Training loss 0.5259650159065071\n",
            "2022-03-31 00:42:01.949128 Epoch 112, Training loss 0.5235126294824474\n",
            "2022-03-31 00:42:12.406089 Epoch 113, Training loss 0.5229764633700061\n",
            "2022-03-31 00:42:22.973729 Epoch 114, Training loss 0.5205682837368583\n",
            "2022-03-31 00:42:33.401049 Epoch 115, Training loss 0.5180332615323688\n",
            "2022-03-31 00:42:43.874916 Epoch 116, Training loss 0.5200554164093169\n",
            "2022-03-31 00:42:54.405871 Epoch 117, Training loss 0.5188003493391949\n",
            "2022-03-31 00:43:04.904568 Epoch 118, Training loss 0.5161829138427134\n",
            "2022-03-31 00:43:15.321513 Epoch 119, Training loss 0.5136373700273921\n",
            "2022-03-31 00:43:26.036989 Epoch 120, Training loss 0.5131149066378698\n",
            "2022-03-31 00:43:36.549361 Epoch 121, Training loss 0.5146825983739265\n",
            "2022-03-31 00:43:47.173175 Epoch 122, Training loss 0.5107266812411415\n",
            "2022-03-31 00:43:57.722622 Epoch 123, Training loss 0.5112719542496954\n",
            "2022-03-31 00:44:08.246267 Epoch 124, Training loss 0.5064815061019204\n",
            "2022-03-31 00:44:18.626721 Epoch 125, Training loss 0.508150960935656\n",
            "2022-03-31 00:44:29.373091 Epoch 126, Training loss 0.5062362770442768\n",
            "2022-03-31 00:44:40.066472 Epoch 127, Training loss 0.5053396589882538\n",
            "2022-03-31 00:44:50.817902 Epoch 128, Training loss 0.5033129560177588\n",
            "2022-03-31 00:45:01.575550 Epoch 129, Training loss 0.5006739671349221\n",
            "2022-03-31 00:45:12.419324 Epoch 130, Training loss 0.500631165836016\n",
            "2022-03-31 00:45:23.229001 Epoch 131, Training loss 0.5001789082194228\n",
            "2022-03-31 00:45:33.999075 Epoch 132, Training loss 0.49733433603783095\n",
            "2022-03-31 00:45:44.628506 Epoch 133, Training loss 0.49773293525895196\n",
            "2022-03-31 00:45:55.305527 Epoch 134, Training loss 0.499549319886643\n",
            "2022-03-31 00:46:05.970242 Epoch 135, Training loss 0.4961825882268074\n",
            "2022-03-31 00:46:16.677808 Epoch 136, Training loss 0.49635073491145887\n",
            "2022-03-31 00:46:27.319440 Epoch 137, Training loss 0.49485517441845306\n",
            "2022-03-31 00:46:37.836009 Epoch 138, Training loss 0.4927372289130755\n",
            "2022-03-31 00:46:48.405289 Epoch 139, Training loss 0.4925639854405847\n",
            "2022-03-31 00:46:58.947810 Epoch 140, Training loss 0.4900169059481767\n",
            "2022-03-31 00:47:09.548437 Epoch 141, Training loss 0.48783139735841385\n",
            "2022-03-31 00:47:20.044002 Epoch 142, Training loss 0.48898303775531254\n",
            "2022-03-31 00:47:30.534241 Epoch 143, Training loss 0.4882812445692699\n",
            "2022-03-31 00:47:41.056432 Epoch 144, Training loss 0.4865567688937382\n",
            "2022-03-31 00:47:51.672502 Epoch 145, Training loss 0.4847249704629869\n",
            "2022-03-31 00:48:02.431297 Epoch 146, Training loss 0.4833972167099833\n",
            "2022-03-31 00:48:12.846715 Epoch 147, Training loss 0.4859147358047383\n",
            "2022-03-31 00:48:23.332803 Epoch 148, Training loss 0.48135835183855824\n",
            "2022-03-31 00:48:33.987528 Epoch 149, Training loss 0.47947633660891475\n",
            "2022-03-31 00:48:44.559569 Epoch 150, Training loss 0.47872562283445197\n",
            "2022-03-31 00:48:55.069686 Epoch 151, Training loss 0.48063755704237676\n",
            "2022-03-31 00:49:05.496333 Epoch 152, Training loss 0.4795244071261047\n",
            "2022-03-31 00:49:16.109286 Epoch 153, Training loss 0.4784500902075597\n",
            "2022-03-31 00:49:26.703540 Epoch 154, Training loss 0.4769160435785113\n",
            "2022-03-31 00:49:37.236793 Epoch 155, Training loss 0.4760576407508472\n",
            "2022-03-31 00:49:47.750811 Epoch 156, Training loss 0.474383258320334\n",
            "2022-03-31 00:49:58.225231 Epoch 157, Training loss 0.47392636346999945\n",
            "2022-03-31 00:50:08.821559 Epoch 158, Training loss 0.47379098078021614\n",
            "2022-03-31 00:50:19.248967 Epoch 159, Training loss 0.47301625058321695\n",
            "2022-03-31 00:50:29.807691 Epoch 160, Training loss 0.47338094968167715\n",
            "2022-03-31 00:50:40.325774 Epoch 161, Training loss 0.46964529265299476\n",
            "2022-03-31 00:50:50.904863 Epoch 162, Training loss 0.46766198697068806\n",
            "2022-03-31 00:51:01.366665 Epoch 163, Training loss 0.470399569196012\n",
            "2022-03-31 00:51:11.839769 Epoch 164, Training loss 0.46745155077151324\n",
            "2022-03-31 00:51:22.244403 Epoch 165, Training loss 0.4671560423186673\n",
            "2022-03-31 00:51:32.725764 Epoch 166, Training loss 0.46673760563135147\n",
            "2022-03-31 00:51:43.263710 Epoch 167, Training loss 0.4649649366469639\n",
            "2022-03-31 00:51:53.879548 Epoch 168, Training loss 0.4642392517736806\n",
            "2022-03-31 00:52:04.456534 Epoch 169, Training loss 0.4652469788708955\n",
            "2022-03-31 00:52:15.036702 Epoch 170, Training loss 0.46305994838095077\n",
            "2022-03-31 00:52:25.576855 Epoch 171, Training loss 0.46390566681428336\n",
            "2022-03-31 00:52:36.067941 Epoch 172, Training loss 0.46306021926957935\n",
            "2022-03-31 00:52:46.689168 Epoch 173, Training loss 0.4610777442793712\n",
            "2022-03-31 00:52:57.142414 Epoch 174, Training loss 0.46018462258455395\n",
            "2022-03-31 00:53:07.744698 Epoch 175, Training loss 0.46082234756111184\n",
            "2022-03-31 00:53:18.267738 Epoch 176, Training loss 0.458427380436979\n",
            "2022-03-31 00:53:28.860158 Epoch 177, Training loss 0.4589604577216346\n",
            "2022-03-31 00:53:39.433738 Epoch 178, Training loss 0.4593773405722645\n",
            "2022-03-31 00:53:49.869955 Epoch 179, Training loss 0.4569064586249459\n",
            "2022-03-31 00:54:00.334608 Epoch 180, Training loss 0.45683347782515504\n",
            "2022-03-31 00:54:10.897164 Epoch 181, Training loss 0.4548346174266332\n",
            "2022-03-31 00:54:21.487132 Epoch 182, Training loss 0.4542335019163463\n",
            "2022-03-31 00:54:32.066225 Epoch 183, Training loss 0.45393043847949915\n",
            "2022-03-31 00:54:42.533154 Epoch 184, Training loss 0.4506057221490099\n",
            "2022-03-31 00:54:53.249132 Epoch 185, Training loss 0.452652749388724\n",
            "2022-03-31 00:55:03.815176 Epoch 186, Training loss 0.4492157508459542\n",
            "2022-03-31 00:55:14.303527 Epoch 187, Training loss 0.4517212615484167\n",
            "2022-03-31 00:55:24.793333 Epoch 188, Training loss 0.4495208492250089\n",
            "2022-03-31 00:55:35.315171 Epoch 189, Training loss 0.4509987606454993\n",
            "2022-03-31 00:55:45.760308 Epoch 190, Training loss 0.4468770715434228\n",
            "2022-03-31 00:55:56.327870 Epoch 191, Training loss 0.4483659723225762\n",
            "2022-03-31 00:56:06.953365 Epoch 192, Training loss 0.44945729360022507\n",
            "2022-03-31 00:56:17.462953 Epoch 193, Training loss 0.44740214661868943\n",
            "2022-03-31 00:56:27.969456 Epoch 194, Training loss 0.44636330494414206\n",
            "2022-03-31 00:56:38.546836 Epoch 195, Training loss 0.44615346345755147\n",
            "2022-03-31 00:56:49.144529 Epoch 196, Training loss 0.44582148789978393\n",
            "2022-03-31 00:56:59.496690 Epoch 197, Training loss 0.44374690734593153\n",
            "2022-03-31 00:57:10.073824 Epoch 198, Training loss 0.4437069957670958\n",
            "2022-03-31 00:57:20.614416 Epoch 199, Training loss 0.443552889825438\n",
            "2022-03-31 00:57:31.269741 Epoch 200, Training loss 0.4433016603445763\n",
            "2022-03-31 00:57:41.724569 Epoch 201, Training loss 0.44414939012978694\n",
            "2022-03-31 00:57:52.196867 Epoch 202, Training loss 0.4433097995417502\n",
            "2022-03-31 00:58:02.705359 Epoch 203, Training loss 0.43960703711223115\n",
            "2022-03-31 00:58:13.074454 Epoch 204, Training loss 0.4407310830738843\n",
            "2022-03-31 00:58:23.710620 Epoch 205, Training loss 0.44055655939728405\n",
            "2022-03-31 00:58:34.180436 Epoch 206, Training loss 0.4395637728197648\n",
            "2022-03-31 00:58:44.797637 Epoch 207, Training loss 0.43767118067159066\n",
            "2022-03-31 00:58:55.309844 Epoch 208, Training loss 0.44140051107120026\n",
            "2022-03-31 00:59:05.709336 Epoch 209, Training loss 0.43935283435427624\n",
            "2022-03-31 00:59:16.178249 Epoch 210, Training loss 0.4375133794706191\n",
            "2022-03-31 00:59:26.858943 Epoch 211, Training loss 0.4372159819430707\n",
            "2022-03-31 00:59:37.384373 Epoch 212, Training loss 0.43472112276974845\n",
            "2022-03-31 00:59:47.908728 Epoch 213, Training loss 0.43596119113514187\n",
            "2022-03-31 00:59:58.501389 Epoch 214, Training loss 0.4354495724372547\n",
            "2022-03-31 01:00:09.042051 Epoch 215, Training loss 0.4359785858208261\n",
            "2022-03-31 01:00:19.566436 Epoch 216, Training loss 0.43474221803114543\n",
            "2022-03-31 01:00:30.044166 Epoch 217, Training loss 0.43238752341026543\n",
            "2022-03-31 01:00:40.413608 Epoch 218, Training loss 0.4348238976029179\n",
            "2022-03-31 01:00:50.955590 Epoch 219, Training loss 0.43169445466354983\n",
            "2022-03-31 01:01:01.607031 Epoch 220, Training loss 0.4314769043984925\n",
            "2022-03-31 01:01:12.161524 Epoch 221, Training loss 0.4297075541427983\n",
            "2022-03-31 01:01:22.767272 Epoch 222, Training loss 0.4304398774833935\n",
            "2022-03-31 01:01:33.582869 Epoch 223, Training loss 0.43072168740546307\n",
            "2022-03-31 01:01:44.155545 Epoch 224, Training loss 0.4299598234083951\n",
            "2022-03-31 01:01:54.669362 Epoch 225, Training loss 0.42857534106811296\n",
            "2022-03-31 01:02:05.235236 Epoch 226, Training loss 0.4289580617109528\n",
            "2022-03-31 01:02:15.685050 Epoch 227, Training loss 0.42806693705756343\n",
            "2022-03-31 01:02:26.484322 Epoch 228, Training loss 0.4271305578825114\n",
            "2022-03-31 01:02:36.998821 Epoch 229, Training loss 0.42786669885487205\n",
            "2022-03-31 01:02:47.391000 Epoch 230, Training loss 0.4259428009581383\n",
            "2022-03-31 01:02:57.786338 Epoch 231, Training loss 0.4258206336165938\n",
            "2022-03-31 01:03:08.213379 Epoch 232, Training loss 0.4245053143109507\n",
            "2022-03-31 01:03:18.671146 Epoch 233, Training loss 0.42675121683065237\n",
            "2022-03-31 01:03:29.173727 Epoch 234, Training loss 0.4261023645548869\n",
            "2022-03-31 01:03:39.472497 Epoch 235, Training loss 0.4237371934458728\n",
            "2022-03-31 01:03:50.042694 Epoch 236, Training loss 0.4249756706454565\n",
            "2022-03-31 01:04:00.725642 Epoch 237, Training loss 0.4227265166428388\n",
            "2022-03-31 01:04:11.216702 Epoch 238, Training loss 0.42414543247969866\n",
            "2022-03-31 01:04:21.706238 Epoch 239, Training loss 0.4209763937632141\n",
            "2022-03-31 01:04:32.245679 Epoch 240, Training loss 0.42252918127019085\n",
            "2022-03-31 01:04:42.826598 Epoch 241, Training loss 0.42066654189468344\n",
            "2022-03-31 01:04:53.460944 Epoch 242, Training loss 0.421393666597431\n",
            "2022-03-31 01:05:04.077116 Epoch 243, Training loss 0.41967975527352996\n",
            "2022-03-31 01:05:14.473257 Epoch 244, Training loss 0.42090914585172673\n",
            "2022-03-31 01:05:24.976186 Epoch 245, Training loss 0.41990230583092747\n",
            "2022-03-31 01:05:35.588969 Epoch 246, Training loss 0.42019500122274583\n",
            "2022-03-31 01:05:46.034978 Epoch 247, Training loss 0.42037074237375915\n",
            "2022-03-31 01:05:56.472897 Epoch 248, Training loss 0.4172830210469873\n",
            "2022-03-31 01:06:07.014940 Epoch 249, Training loss 0.4192665886619817\n",
            "2022-03-31 01:06:17.437354 Epoch 250, Training loss 0.4167250771732891\n",
            "2022-03-31 01:06:27.914018 Epoch 251, Training loss 0.41808350832032426\n",
            "2022-03-31 01:06:38.516352 Epoch 252, Training loss 0.41617257860691653\n",
            "2022-03-31 01:06:48.988138 Epoch 253, Training loss 0.41490136168878095\n",
            "2022-03-31 01:06:59.462520 Epoch 254, Training loss 0.4162251682728148\n",
            "2022-03-31 01:07:09.814907 Epoch 255, Training loss 0.41730373299411494\n",
            "2022-03-31 01:07:20.385249 Epoch 256, Training loss 0.4176465477556219\n",
            "2022-03-31 01:07:30.912728 Epoch 257, Training loss 0.41166070084590134\n",
            "2022-03-31 01:07:41.498517 Epoch 258, Training loss 0.4165045052881131\n",
            "2022-03-31 01:07:52.084982 Epoch 259, Training loss 0.41185853323515725\n",
            "2022-03-31 01:08:03.062696 Epoch 260, Training loss 0.41310589604289333\n",
            "2022-03-31 01:08:13.655459 Epoch 261, Training loss 0.413046371830089\n",
            "2022-03-31 01:08:24.334111 Epoch 262, Training loss 0.4139134168929761\n",
            "2022-03-31 01:08:35.059690 Epoch 263, Training loss 0.4112760107726087\n",
            "2022-03-31 01:08:45.506553 Epoch 264, Training loss 0.4113971370146098\n",
            "2022-03-31 01:08:56.029952 Epoch 265, Training loss 0.41101214445917805\n",
            "2022-03-31 01:09:06.451287 Epoch 266, Training loss 0.4113404102185193\n",
            "2022-03-31 01:09:17.070670 Epoch 267, Training loss 0.4094889445225601\n",
            "2022-03-31 01:09:27.659067 Epoch 268, Training loss 0.40761918671753095\n",
            "2022-03-31 01:09:38.210244 Epoch 269, Training loss 0.4109351720727618\n",
            "2022-03-31 01:09:48.748244 Epoch 270, Training loss 0.40936828101687417\n",
            "2022-03-31 01:09:59.214341 Epoch 271, Training loss 0.40830724192854695\n",
            "2022-03-31 01:10:09.902950 Epoch 272, Training loss 0.4099038769026547\n",
            "2022-03-31 01:10:20.766770 Epoch 273, Training loss 0.4094703707777326\n",
            "2022-03-31 01:10:31.342992 Epoch 274, Training loss 0.40924357390388505\n",
            "2022-03-31 01:10:41.870493 Epoch 275, Training loss 0.40756177973678653\n",
            "2022-03-31 01:10:52.512017 Epoch 276, Training loss 0.4060940948669868\n",
            "2022-03-31 01:11:03.159176 Epoch 277, Training loss 0.4065055163086528\n",
            "2022-03-31 01:11:13.597448 Epoch 278, Training loss 0.4058425482505423\n",
            "2022-03-31 01:11:24.105957 Epoch 279, Training loss 0.40387235734316396\n",
            "2022-03-31 01:11:34.659537 Epoch 280, Training loss 0.40860657533035255\n",
            "2022-03-31 01:11:45.220404 Epoch 281, Training loss 0.40331936835328025\n",
            "2022-03-31 01:11:55.637621 Epoch 282, Training loss 0.4057479553744006\n",
            "2022-03-31 01:12:06.179861 Epoch 283, Training loss 0.40507022741124454\n",
            "2022-03-31 01:12:16.770360 Epoch 284, Training loss 0.4045936423913597\n",
            "2022-03-31 01:12:27.240188 Epoch 285, Training loss 0.4021142761763709\n",
            "2022-03-31 01:12:37.693196 Epoch 286, Training loss 0.40677938242550093\n",
            "2022-03-31 01:12:48.321663 Epoch 287, Training loss 0.40530641902895537\n",
            "2022-03-31 01:12:58.771341 Epoch 288, Training loss 0.40203899596734427\n",
            "2022-03-31 01:13:09.350582 Epoch 289, Training loss 0.4050906394486842\n",
            "2022-03-31 01:13:19.916395 Epoch 290, Training loss 0.40253111984952333\n",
            "2022-03-31 01:13:30.382504 Epoch 291, Training loss 0.4012523439457959\n",
            "2022-03-31 01:13:40.760536 Epoch 292, Training loss 0.4010711494080551\n",
            "2022-03-31 01:13:51.251177 Epoch 293, Training loss 0.4023464056651306\n",
            "2022-03-31 01:14:01.697051 Epoch 294, Training loss 0.39901111025334624\n",
            "2022-03-31 01:14:12.077528 Epoch 295, Training loss 0.40132886396192224\n",
            "2022-03-31 01:14:22.538517 Epoch 296, Training loss 0.40271975532593324\n",
            "2022-03-31 01:14:32.916520 Epoch 297, Training loss 0.3987147576887818\n",
            "2022-03-31 01:14:43.389799 Epoch 298, Training loss 0.39959683032977916\n",
            "2022-03-31 01:14:53.951349 Epoch 299, Training loss 0.3986646807407174\n",
            "2022-03-31 01:15:04.320463 Epoch 300, Training loss 0.40251396562132385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7DV8TILVS5K",
        "outputId": "66d3aeab-a9eb-45c0-a6df-ef9b7771a015"
      },
      "id": "Q7DV8TILVS5K",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.84\n",
            "Accuracy val: 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetBatchNorm(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
        "        self.fc1 = nn.Linear(8*8*n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1_batchnorm(self.conv1(x))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = self.conv2_batchnorm(self.conv2(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = out.view(-1, 8*8*self.n_chans1 //2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "qWEqG0DzgyrO"
      },
      "id": "qWEqG0DzgyrO",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8R_CNgfg7Vq",
        "outputId": "a150e8e7-a55e-43d8-b067-5dc14c3cb3ab"
      },
      "id": "f8R_CNgfg7Vq",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-31 01:15:47.432040 Epoch 1, Training loss 2.127211928672498\n",
            "2022-03-31 01:15:57.936351 Epoch 2, Training loss 1.743445701763758\n",
            "2022-03-31 01:16:08.470747 Epoch 3, Training loss 1.5428482315424459\n",
            "2022-03-31 01:16:19.086349 Epoch 4, Training loss 1.4376540674882776\n",
            "2022-03-31 01:16:29.596830 Epoch 5, Training loss 1.3556002857130203\n",
            "2022-03-31 01:16:39.995035 Epoch 6, Training loss 1.2874773601284417\n",
            "2022-03-31 01:16:50.369154 Epoch 7, Training loss 1.2281443213715273\n",
            "2022-03-31 01:17:00.887716 Epoch 8, Training loss 1.1787294968772117\n",
            "2022-03-31 01:17:11.287878 Epoch 9, Training loss 1.1350381022219158\n",
            "2022-03-31 01:17:21.653178 Epoch 10, Training loss 1.0955347188598359\n",
            "2022-03-31 01:17:32.104907 Epoch 11, Training loss 1.0598252476633663\n",
            "2022-03-31 01:17:42.505809 Epoch 12, Training loss 1.02748018297393\n",
            "2022-03-31 01:17:53.037361 Epoch 13, Training loss 1.0022700105786628\n",
            "2022-03-31 01:18:03.566055 Epoch 14, Training loss 0.9815815787028779\n",
            "2022-03-31 01:18:13.902169 Epoch 15, Training loss 0.9596670741010505\n",
            "2022-03-31 01:18:24.369725 Epoch 16, Training loss 0.9392179256052617\n",
            "2022-03-31 01:18:34.960357 Epoch 17, Training loss 0.9222053999028852\n",
            "2022-03-31 01:18:45.519172 Epoch 18, Training loss 0.9058398954246355\n",
            "2022-03-31 01:18:55.962457 Epoch 19, Training loss 0.8922347440896436\n",
            "2022-03-31 01:19:06.442850 Epoch 20, Training loss 0.8785586593401097\n",
            "2022-03-31 01:19:16.835105 Epoch 21, Training loss 0.8659396844599253\n",
            "2022-03-31 01:19:27.208018 Epoch 22, Training loss 0.8543123804852176\n",
            "2022-03-31 01:19:37.670666 Epoch 23, Training loss 0.8436284483698628\n",
            "2022-03-31 01:19:48.179478 Epoch 24, Training loss 0.8300492386226459\n",
            "2022-03-31 01:19:58.806555 Epoch 25, Training loss 0.820790208475974\n",
            "2022-03-31 01:20:09.321779 Epoch 26, Training loss 0.811704608790405\n",
            "2022-03-31 01:20:19.832925 Epoch 27, Training loss 0.8029007779057983\n",
            "2022-03-31 01:20:30.224758 Epoch 28, Training loss 0.795248840775941\n",
            "2022-03-31 01:20:40.630812 Epoch 29, Training loss 0.7839290535129855\n",
            "2022-03-31 01:20:51.140392 Epoch 30, Training loss 0.7788900848087448\n",
            "2022-03-31 01:21:01.575669 Epoch 31, Training loss 0.7692148358468205\n",
            "2022-03-31 01:21:12.092715 Epoch 32, Training loss 0.7641208888700856\n",
            "2022-03-31 01:21:22.644832 Epoch 33, Training loss 0.756465062956371\n",
            "2022-03-31 01:21:33.450199 Epoch 34, Training loss 0.7484507221166435\n",
            "2022-03-31 01:21:44.001957 Epoch 35, Training loss 0.7442817097277288\n",
            "2022-03-31 01:21:54.424371 Epoch 36, Training loss 0.7376982684406783\n",
            "2022-03-31 01:22:04.882193 Epoch 37, Training loss 0.7334662076000058\n",
            "2022-03-31 01:22:15.535073 Epoch 38, Training loss 0.7269118036455511\n",
            "2022-03-31 01:22:26.047759 Epoch 39, Training loss 0.7184947878503434\n",
            "2022-03-31 01:22:36.534193 Epoch 40, Training loss 0.7160029868640558\n",
            "2022-03-31 01:22:46.976299 Epoch 41, Training loss 0.712130396148128\n",
            "2022-03-31 01:22:57.500405 Epoch 42, Training loss 0.7072915161204765\n",
            "2022-03-31 01:23:08.092224 Epoch 43, Training loss 0.7017487021129759\n",
            "2022-03-31 01:23:18.468827 Epoch 44, Training loss 0.6956912903758266\n",
            "2022-03-31 01:23:29.019413 Epoch 45, Training loss 0.6933362063239602\n",
            "2022-03-31 01:23:39.565835 Epoch 46, Training loss 0.6885338915735865\n",
            "2022-03-31 01:23:50.168584 Epoch 47, Training loss 0.6844387451553589\n",
            "2022-03-31 01:24:00.738513 Epoch 48, Training loss 0.6798467388009781\n",
            "2022-03-31 01:24:11.356633 Epoch 49, Training loss 0.6760806686356854\n",
            "2022-03-31 01:24:21.861207 Epoch 50, Training loss 0.6702265170071741\n",
            "2022-03-31 01:24:32.436019 Epoch 51, Training loss 0.6706258153823941\n",
            "2022-03-31 01:24:42.902172 Epoch 52, Training loss 0.6674010827565742\n",
            "2022-03-31 01:24:53.485915 Epoch 53, Training loss 0.6625921382090015\n",
            "2022-03-31 01:25:03.991833 Epoch 54, Training loss 0.6596285896685422\n",
            "2022-03-31 01:25:14.611068 Epoch 55, Training loss 0.6562252670831388\n",
            "2022-03-31 01:25:25.092547 Epoch 56, Training loss 0.6509001834694382\n",
            "2022-03-31 01:25:35.628175 Epoch 57, Training loss 0.6491618366040233\n",
            "2022-03-31 01:25:46.021169 Epoch 58, Training loss 0.6468137755723256\n",
            "2022-03-31 01:25:56.469991 Epoch 59, Training loss 0.6426563400136845\n",
            "2022-03-31 01:26:06.967299 Epoch 60, Training loss 0.6402362554198335\n",
            "2022-03-31 01:26:17.438373 Epoch 61, Training loss 0.6342562613508586\n",
            "2022-03-31 01:26:28.002394 Epoch 62, Training loss 0.6371041768042328\n",
            "2022-03-31 01:26:38.492184 Epoch 63, Training loss 0.6320510643827336\n",
            "2022-03-31 01:26:49.024744 Epoch 64, Training loss 0.6306574718879007\n",
            "2022-03-31 01:26:59.388857 Epoch 65, Training loss 0.6240008697485375\n",
            "2022-03-31 01:27:09.907741 Epoch 66, Training loss 0.6229303157542978\n",
            "2022-03-31 01:27:20.362993 Epoch 67, Training loss 0.6227337702003586\n",
            "2022-03-31 01:27:30.788182 Epoch 68, Training loss 0.6183027676151841\n",
            "2022-03-31 01:27:41.090342 Epoch 69, Training loss 0.6151587864017243\n",
            "2022-03-31 01:27:51.557968 Epoch 70, Training loss 0.6154222450292933\n",
            "2022-03-31 01:28:02.075203 Epoch 71, Training loss 0.6113141883555275\n",
            "2022-03-31 01:28:12.488588 Epoch 72, Training loss 0.6097540113017382\n",
            "2022-03-31 01:28:23.132679 Epoch 73, Training loss 0.6070684194564819\n",
            "2022-03-31 01:28:33.562297 Epoch 74, Training loss 0.6067363893436959\n",
            "2022-03-31 01:28:43.936710 Epoch 75, Training loss 0.5997526530567032\n",
            "2022-03-31 01:28:54.469852 Epoch 76, Training loss 0.6017058693310794\n",
            "2022-03-31 01:29:04.918086 Epoch 77, Training loss 0.5985229319852331\n",
            "2022-03-31 01:29:15.238037 Epoch 78, Training loss 0.5969281579794177\n",
            "2022-03-31 01:29:25.755015 Epoch 79, Training loss 0.5940876896576504\n",
            "2022-03-31 01:29:36.284067 Epoch 80, Training loss 0.5912868618355382\n",
            "2022-03-31 01:29:46.726335 Epoch 81, Training loss 0.5918176773640201\n",
            "2022-03-31 01:29:57.358056 Epoch 82, Training loss 0.5871937499403039\n",
            "2022-03-31 01:30:07.769773 Epoch 83, Training loss 0.5858293295935597\n",
            "2022-03-31 01:30:18.250892 Epoch 84, Training loss 0.5850230666530102\n",
            "2022-03-31 01:30:28.886894 Epoch 85, Training loss 0.5818408599213871\n",
            "2022-03-31 01:30:39.328461 Epoch 86, Training loss 0.5796990545509416\n",
            "2022-03-31 01:30:49.646249 Epoch 87, Training loss 0.5780051445107326\n",
            "2022-03-31 01:31:00.105779 Epoch 88, Training loss 0.5791784314548268\n",
            "2022-03-31 01:31:10.700842 Epoch 89, Training loss 0.5768372404682057\n",
            "2022-03-31 01:31:21.030815 Epoch 90, Training loss 0.5718798515818003\n",
            "2022-03-31 01:31:31.572944 Epoch 91, Training loss 0.5702716438933406\n",
            "2022-03-31 01:31:42.051939 Epoch 92, Training loss 0.5684702860577332\n",
            "2022-03-31 01:31:52.575962 Epoch 93, Training loss 0.5664953031693883\n",
            "2022-03-31 01:32:03.030269 Epoch 94, Training loss 0.5667843902507401\n",
            "2022-03-31 01:32:13.476705 Epoch 95, Training loss 0.5637545663758617\n",
            "2022-03-31 01:32:24.035199 Epoch 96, Training loss 0.5621372327170409\n",
            "2022-03-31 01:32:34.513019 Epoch 97, Training loss 0.5609099718615832\n",
            "2022-03-31 01:32:44.883523 Epoch 98, Training loss 0.5584610669166231\n",
            "2022-03-31 01:32:55.478802 Epoch 99, Training loss 0.5587648052693633\n",
            "2022-03-31 01:33:06.008013 Epoch 100, Training loss 0.5553122941033005\n",
            "2022-03-31 01:33:16.389696 Epoch 101, Training loss 0.5553196237596405\n",
            "2022-03-31 01:33:26.950422 Epoch 102, Training loss 0.5515458308865347\n",
            "2022-03-31 01:33:37.619479 Epoch 103, Training loss 0.5526545225735515\n",
            "2022-03-31 01:33:48.166606 Epoch 104, Training loss 0.5496280500308022\n",
            "2022-03-31 01:33:58.577098 Epoch 105, Training loss 0.5478899664319384\n",
            "2022-03-31 01:34:09.117674 Epoch 106, Training loss 0.5468369991235111\n",
            "2022-03-31 01:34:19.482756 Epoch 107, Training loss 0.5442819240529214\n",
            "2022-03-31 01:34:30.014446 Epoch 108, Training loss 0.5437254259920181\n",
            "2022-03-31 01:34:40.364082 Epoch 109, Training loss 0.540217100773626\n",
            "2022-03-31 01:34:50.743039 Epoch 110, Training loss 0.5406941379351384\n",
            "2022-03-31 01:35:01.282211 Epoch 111, Training loss 0.5390471815110167\n",
            "2022-03-31 01:35:11.767693 Epoch 112, Training loss 0.5387360329365791\n",
            "2022-03-31 01:35:22.170465 Epoch 113, Training loss 0.5365817487392279\n",
            "2022-03-31 01:35:32.620025 Epoch 114, Training loss 0.5330211735137588\n",
            "2022-03-31 01:35:43.072590 Epoch 115, Training loss 0.5315034230003881\n",
            "2022-03-31 01:35:53.408051 Epoch 116, Training loss 0.5315000162176464\n",
            "2022-03-31 01:36:03.844470 Epoch 117, Training loss 0.5292333457285486\n",
            "2022-03-31 01:36:14.448196 Epoch 118, Training loss 0.5297488243035648\n",
            "2022-03-31 01:36:25.115154 Epoch 119, Training loss 0.5263929006922275\n",
            "2022-03-31 01:36:35.624848 Epoch 120, Training loss 0.5269413021824244\n",
            "2022-03-31 01:36:46.077005 Epoch 121, Training loss 0.5230308566290094\n",
            "2022-03-31 01:36:56.494007 Epoch 122, Training loss 0.5223533497060961\n",
            "2022-03-31 01:37:06.965998 Epoch 123, Training loss 0.5192448891642149\n",
            "2022-03-31 01:37:17.318014 Epoch 124, Training loss 0.5176403362046728\n",
            "2022-03-31 01:37:27.688112 Epoch 125, Training loss 0.519410560179092\n",
            "2022-03-31 01:37:38.200344 Epoch 126, Training loss 0.5191680536321972\n",
            "2022-03-31 01:37:48.625856 Epoch 127, Training loss 0.5164879568854867\n",
            "2022-03-31 01:37:59.127398 Epoch 128, Training loss 0.5154947749793987\n",
            "2022-03-31 01:38:09.483639 Epoch 129, Training loss 0.5098160941658727\n",
            "2022-03-31 01:38:19.961246 Epoch 130, Training loss 0.5128453519490673\n",
            "2022-03-31 01:38:30.526185 Epoch 131, Training loss 0.5082198973087704\n",
            "2022-03-31 01:38:41.112858 Epoch 132, Training loss 0.5083368334090314\n",
            "2022-03-31 01:38:51.496528 Epoch 133, Training loss 0.5076228072842979\n",
            "2022-03-31 01:39:01.962056 Epoch 134, Training loss 0.506143488142344\n",
            "2022-03-31 01:39:12.507983 Epoch 135, Training loss 0.5045451404874587\n",
            "2022-03-31 01:39:23.067206 Epoch 136, Training loss 0.50434896603341\n",
            "2022-03-31 01:39:33.610940 Epoch 137, Training loss 0.5019007401202645\n",
            "2022-03-31 01:39:44.097507 Epoch 138, Training loss 0.5018193116768852\n",
            "2022-03-31 01:39:54.648111 Epoch 139, Training loss 0.499934634855946\n",
            "2022-03-31 01:40:05.163035 Epoch 140, Training loss 0.49904547513598374\n",
            "2022-03-31 01:40:15.787538 Epoch 141, Training loss 0.4961309878112715\n",
            "2022-03-31 01:40:26.271812 Epoch 142, Training loss 0.49670243145102433\n",
            "2022-03-31 01:40:36.815235 Epoch 143, Training loss 0.4950081282069006\n",
            "2022-03-31 01:40:47.190296 Epoch 144, Training loss 0.4942994504747793\n",
            "2022-03-31 01:40:57.755417 Epoch 145, Training loss 0.49289600989397836\n",
            "2022-03-31 01:41:08.260742 Epoch 146, Training loss 0.4916098748555269\n",
            "2022-03-31 01:41:18.774999 Epoch 147, Training loss 0.4899252229525\n",
            "2022-03-31 01:41:29.254375 Epoch 148, Training loss 0.4883273978862921\n",
            "2022-03-31 01:41:39.586560 Epoch 149, Training loss 0.4873244834068181\n",
            "2022-03-31 01:41:50.133049 Epoch 150, Training loss 0.48689490509078936\n",
            "2022-03-31 01:42:00.528423 Epoch 151, Training loss 0.4834632047492525\n",
            "2022-03-31 01:42:10.930249 Epoch 152, Training loss 0.4822103132867752\n",
            "2022-03-31 01:42:21.930498 Epoch 153, Training loss 0.48312210440254577\n",
            "2022-03-31 01:42:32.493840 Epoch 154, Training loss 0.48185231338452805\n",
            "2022-03-31 01:42:43.540182 Epoch 155, Training loss 0.48010598791910863\n",
            "2022-03-31 01:42:54.570573 Epoch 156, Training loss 0.4789440849095659\n",
            "2022-03-31 01:43:05.078585 Epoch 157, Training loss 0.47967571214489313\n",
            "2022-03-31 01:43:15.703478 Epoch 158, Training loss 0.4785425945773454\n",
            "2022-03-31 01:43:26.212356 Epoch 159, Training loss 0.4738363773392899\n",
            "2022-03-31 01:43:36.683807 Epoch 160, Training loss 0.4739178287632325\n",
            "2022-03-31 01:43:47.194217 Epoch 161, Training loss 0.47516507939304536\n",
            "2022-03-31 01:43:57.784141 Epoch 162, Training loss 0.4730996415781243\n",
            "2022-03-31 01:44:08.173670 Epoch 163, Training loss 0.46961478035315835\n",
            "2022-03-31 01:44:18.666138 Epoch 164, Training loss 0.4702673757929936\n",
            "2022-03-31 01:44:29.193125 Epoch 165, Training loss 0.4701722457509516\n",
            "2022-03-31 01:44:39.650201 Epoch 166, Training loss 0.46995983377594475\n",
            "2022-03-31 01:44:50.130950 Epoch 167, Training loss 0.4674829513291874\n",
            "2022-03-31 01:45:00.588962 Epoch 168, Training loss 0.46671036388867954\n",
            "2022-03-31 01:45:11.021058 Epoch 169, Training loss 0.46377701643863906\n",
            "2022-03-31 01:45:21.437063 Epoch 170, Training loss 0.4662710068475865\n",
            "2022-03-31 01:45:32.053826 Epoch 171, Training loss 0.46562179183716057\n",
            "2022-03-31 01:45:42.602069 Epoch 172, Training loss 0.46299704713056156\n",
            "2022-03-31 01:45:53.123228 Epoch 173, Training loss 0.45905506313609346\n",
            "2022-03-31 01:46:03.586236 Epoch 174, Training loss 0.4590907243778334\n",
            "2022-03-31 01:46:13.974193 Epoch 175, Training loss 0.45884743047034954\n",
            "2022-03-31 01:46:24.319187 Epoch 176, Training loss 0.4590328847012861\n",
            "2022-03-31 01:46:34.823854 Epoch 177, Training loss 0.4575857310305776\n",
            "2022-03-31 01:46:45.197447 Epoch 178, Training loss 0.45739934864022846\n",
            "2022-03-31 01:46:55.580221 Epoch 179, Training loss 0.45476956034789\n",
            "2022-03-31 01:47:05.997661 Epoch 180, Training loss 0.4549877906356321\n",
            "2022-03-31 01:47:16.392660 Epoch 181, Training loss 0.45240588874920556\n",
            "2022-03-31 01:47:26.761136 Epoch 182, Training loss 0.453069819799622\n",
            "2022-03-31 01:47:37.177534 Epoch 183, Training loss 0.4526064484130086\n",
            "2022-03-31 01:47:47.538844 Epoch 184, Training loss 0.4515497952775882\n",
            "2022-03-31 01:47:57.978058 Epoch 185, Training loss 0.44797149361551875\n",
            "2022-03-31 01:48:08.486770 Epoch 186, Training loss 0.44824310024376113\n",
            "2022-03-31 01:48:19.060488 Epoch 187, Training loss 0.44954820188796124\n",
            "2022-03-31 01:48:29.522132 Epoch 188, Training loss 0.4473834334851226\n",
            "2022-03-31 01:48:39.996918 Epoch 189, Training loss 0.4459229373870908\n",
            "2022-03-31 01:48:50.380137 Epoch 190, Training loss 0.44563112714711356\n",
            "2022-03-31 01:49:00.843859 Epoch 191, Training loss 0.4451262211936819\n",
            "2022-03-31 01:49:11.339003 Epoch 192, Training loss 0.44478156662467494\n",
            "2022-03-31 01:49:21.864345 Epoch 193, Training loss 0.44143284864895177\n",
            "2022-03-31 01:49:32.281839 Epoch 194, Training loss 0.44355723399030583\n",
            "2022-03-31 01:49:42.704118 Epoch 195, Training loss 0.4411038612313283\n",
            "2022-03-31 01:49:53.175556 Epoch 196, Training loss 0.4391749337734774\n",
            "2022-03-31 01:50:03.745525 Epoch 197, Training loss 0.4395777755762305\n",
            "2022-03-31 01:50:14.105320 Epoch 198, Training loss 0.4417021966460721\n",
            "2022-03-31 01:50:24.488115 Epoch 199, Training loss 0.4390196426178488\n",
            "2022-03-31 01:50:35.090896 Epoch 200, Training loss 0.4401891711156082\n",
            "2022-03-31 01:50:45.668569 Epoch 201, Training loss 0.4372889172581151\n",
            "2022-03-31 01:50:56.298344 Epoch 202, Training loss 0.43766499289771177\n",
            "2022-03-31 01:51:06.908208 Epoch 203, Training loss 0.43578037087950866\n",
            "2022-03-31 01:51:17.328299 Epoch 204, Training loss 0.43509231286737926\n",
            "2022-03-31 01:51:27.860090 Epoch 205, Training loss 0.43816272205556445\n",
            "2022-03-31 01:51:38.334592 Epoch 206, Training loss 0.43367774401555587\n",
            "2022-03-31 01:51:48.890386 Epoch 207, Training loss 0.4303817241202535\n",
            "2022-03-31 01:51:59.520697 Epoch 208, Training loss 0.4309077493065154\n",
            "2022-03-31 01:52:10.251872 Epoch 209, Training loss 0.43100709551969146\n",
            "2022-03-31 01:52:20.828909 Epoch 210, Training loss 0.4342329261438621\n",
            "2022-03-31 01:52:31.398689 Epoch 211, Training loss 0.4306591661537395\n",
            "2022-03-31 01:52:41.997432 Epoch 212, Training loss 0.4322967963945835\n",
            "2022-03-31 01:52:52.433852 Epoch 213, Training loss 0.42889508664074455\n",
            "2022-03-31 01:53:02.871378 Epoch 214, Training loss 0.4245928399779303\n",
            "2022-03-31 01:53:13.388238 Epoch 215, Training loss 0.42748090580982323\n",
            "2022-03-31 01:53:23.771501 Epoch 216, Training loss 0.4294108379717983\n",
            "2022-03-31 01:53:34.378106 Epoch 217, Training loss 0.4241765541646182\n",
            "2022-03-31 01:53:44.880182 Epoch 218, Training loss 0.4237267576786868\n",
            "2022-03-31 01:53:55.299322 Epoch 219, Training loss 0.4269807080707282\n",
            "2022-03-31 01:54:05.775415 Epoch 220, Training loss 0.42254823579660156\n",
            "2022-03-31 01:54:16.182771 Epoch 221, Training loss 0.4249971825867663\n",
            "2022-03-31 01:54:26.581674 Epoch 222, Training loss 0.4242044031200811\n",
            "2022-03-31 01:54:37.092655 Epoch 223, Training loss 0.4230374354497551\n",
            "2022-03-31 01:54:47.562094 Epoch 224, Training loss 0.4206818488362195\n",
            "2022-03-31 01:54:58.102810 Epoch 225, Training loss 0.42101889732472425\n",
            "2022-03-31 01:55:08.570296 Epoch 226, Training loss 0.42013167338374324\n",
            "2022-03-31 01:55:19.072566 Epoch 227, Training loss 0.4187081034111855\n",
            "2022-03-31 01:55:29.460070 Epoch 228, Training loss 0.41799270986672254\n",
            "2022-03-31 01:55:39.943869 Epoch 229, Training loss 0.4188341206258825\n",
            "2022-03-31 01:55:50.507732 Epoch 230, Training loss 0.42330188425186344\n",
            "2022-03-31 01:56:00.952493 Epoch 231, Training loss 0.4183209764073267\n",
            "2022-03-31 01:56:11.515518 Epoch 232, Training loss 0.4157633979797668\n",
            "2022-03-31 01:56:22.104060 Epoch 233, Training loss 0.4179237537714831\n",
            "2022-03-31 01:56:32.495105 Epoch 234, Training loss 0.41621560283252956\n",
            "2022-03-31 01:56:42.867550 Epoch 235, Training loss 0.4160313323292586\n",
            "2022-03-31 01:56:53.276216 Epoch 236, Training loss 0.4167220590021604\n",
            "2022-03-31 01:57:03.716039 Epoch 237, Training loss 0.41492915675615716\n",
            "2022-03-31 01:57:14.091459 Epoch 238, Training loss 0.41488909422207976\n",
            "2022-03-31 01:57:24.592640 Epoch 239, Training loss 0.4120306346918006\n",
            "2022-03-31 01:57:35.181111 Epoch 240, Training loss 0.41326851212917387\n",
            "2022-03-31 01:57:45.602501 Epoch 241, Training loss 0.4113494142737535\n",
            "2022-03-31 01:57:56.129899 Epoch 242, Training loss 0.41337007464236003\n",
            "2022-03-31 01:58:06.517628 Epoch 243, Training loss 0.41426662008856874\n",
            "2022-03-31 01:58:16.892205 Epoch 244, Training loss 0.4094187428655527\n",
            "2022-03-31 01:58:27.379032 Epoch 245, Training loss 0.4125662859138625\n",
            "2022-03-31 01:58:37.864935 Epoch 246, Training loss 0.40481374231750705\n",
            "2022-03-31 01:58:48.414252 Epoch 247, Training loss 0.4069735099325704\n",
            "2022-03-31 01:58:58.857447 Epoch 248, Training loss 0.4075282432348527\n",
            "2022-03-31 01:59:09.278966 Epoch 249, Training loss 0.41288908205145153\n",
            "2022-03-31 01:59:19.862810 Epoch 250, Training loss 0.4059986744237983\n",
            "2022-03-31 01:59:30.442321 Epoch 251, Training loss 0.4037180877173953\n",
            "2022-03-31 01:59:40.839348 Epoch 252, Training loss 0.40438893167754575\n",
            "2022-03-31 01:59:51.333909 Epoch 253, Training loss 0.4054780536143066\n",
            "2022-03-31 02:00:01.864623 Epoch 254, Training loss 0.40670184463338777\n",
            "2022-03-31 02:00:12.361534 Epoch 255, Training loss 0.40243491444670026\n",
            "2022-03-31 02:00:22.760442 Epoch 256, Training loss 0.4040213692790407\n",
            "2022-03-31 02:00:33.380403 Epoch 257, Training loss 0.40365858682814765\n",
            "2022-03-31 02:00:43.874043 Epoch 258, Training loss 0.40297040135583\n",
            "2022-03-31 02:00:54.315097 Epoch 259, Training loss 0.4016587207346316\n",
            "2022-03-31 02:01:04.860014 Epoch 260, Training loss 0.4026637775132723\n",
            "2022-03-31 02:01:15.399044 Epoch 261, Training loss 0.39965970764684555\n",
            "2022-03-31 02:01:25.855560 Epoch 262, Training loss 0.4031096480386641\n",
            "2022-03-31 02:01:36.495181 Epoch 263, Training loss 0.398839766859932\n",
            "2022-03-31 02:01:47.152220 Epoch 264, Training loss 0.39865433721972243\n",
            "2022-03-31 02:01:57.685716 Epoch 265, Training loss 0.39944821192175534\n",
            "2022-03-31 02:02:08.205217 Epoch 266, Training loss 0.40142249295016386\n",
            "2022-03-31 02:02:18.539192 Epoch 267, Training loss 0.40365294521422035\n",
            "2022-03-31 02:02:28.939471 Epoch 268, Training loss 0.3960916223885763\n",
            "2022-03-31 02:02:39.341514 Epoch 269, Training loss 0.39558439454078065\n",
            "2022-03-31 02:02:49.812001 Epoch 270, Training loss 0.3956126818419113\n",
            "2022-03-31 02:03:00.117345 Epoch 271, Training loss 0.39388407406675846\n",
            "2022-03-31 02:03:10.580684 Epoch 272, Training loss 0.39538495318816447\n",
            "2022-03-31 02:03:21.026899 Epoch 273, Training loss 0.393389477849464\n",
            "2022-03-31 02:03:31.522467 Epoch 274, Training loss 0.39776421858526556\n",
            "2022-03-31 02:03:41.937608 Epoch 275, Training loss 0.39481541573467765\n",
            "2022-03-31 02:03:52.443832 Epoch 276, Training loss 0.39280398170966324\n",
            "2022-03-31 02:04:02.898349 Epoch 277, Training loss 0.3921842228649827\n",
            "2022-03-31 02:04:13.333525 Epoch 278, Training loss 0.39754336487378\n",
            "2022-03-31 02:04:23.849785 Epoch 279, Training loss 0.390088007216106\n",
            "2022-03-31 02:04:34.485914 Epoch 280, Training loss 0.39143552162382\n",
            "2022-03-31 02:04:45.051113 Epoch 281, Training loss 0.38953897077828414\n",
            "2022-03-31 02:04:55.589725 Epoch 282, Training loss 0.3936524859741521\n",
            "2022-03-31 02:05:06.088269 Epoch 283, Training loss 0.39128704348107435\n",
            "2022-03-31 02:05:16.814120 Epoch 284, Training loss 0.39336192442099455\n",
            "2022-03-31 02:05:27.420894 Epoch 285, Training loss 0.3897571596495636\n",
            "2022-03-31 02:05:37.994898 Epoch 286, Training loss 0.39010051441619464\n",
            "2022-03-31 02:05:48.522579 Epoch 287, Training loss 0.3881440158275997\n",
            "2022-03-31 02:05:59.021502 Epoch 288, Training loss 0.38973422449491824\n",
            "2022-03-31 02:06:09.589448 Epoch 289, Training loss 0.3871743412273924\n",
            "2022-03-31 02:06:20.085336 Epoch 290, Training loss 0.390175348428814\n",
            "2022-03-31 02:06:30.684052 Epoch 291, Training loss 0.39395871880414235\n",
            "2022-03-31 02:06:41.319839 Epoch 292, Training loss 0.3873099166509288\n",
            "2022-03-31 02:06:51.881997 Epoch 293, Training loss 0.38840696446197415\n",
            "2022-03-31 02:07:02.363558 Epoch 294, Training loss 0.38601555347518846\n",
            "2022-03-31 02:07:12.948611 Epoch 295, Training loss 0.3843317426112302\n",
            "2022-03-31 02:07:23.509012 Epoch 296, Training loss 0.3893877186471849\n",
            "2022-03-31 02:07:33.901836 Epoch 297, Training loss 0.38500403696694946\n",
            "2022-03-31 02:07:44.372265 Epoch 298, Training loss 0.38501514993665165\n",
            "2022-03-31 02:07:54.850638 Epoch 299, Training loss 0.38534886597672385\n",
            "2022-03-31 02:08:05.303815 Epoch 300, Training loss 0.3840034678387825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLWZcmSfhKiq",
        "outputId": "f70c65ca-59d6-42ca-a57b-ec734e9d13f1"
      },
      "id": "MLWZcmSfhKiq",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.83\n",
            "Accuracy val: 0.69\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a71fd439aa9940ddb1fdd20d64764059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5635eba58f824c2288c47754f5de6b45",
              "IPY_MODEL_e2f51453be7a44bd940f8b2eb2ef5d7b",
              "IPY_MODEL_5c2e5d5f5b7e4e248c825d20e7f75c84"
            ],
            "layout": "IPY_MODEL_8bb190912b114fa5858c4d5ec10a364c"
          }
        },
        "5635eba58f824c2288c47754f5de6b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc8d0ee7bf4487d930487ca2899d5d0",
            "placeholder": "",
            "style": "IPY_MODEL_11579493c8244feabdbcce11555b8687",
            "value": ""
          }
        },
        "e2f51453be7a44bd940f8b2eb2ef5d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26bca5a0e4b54cd4855bdaf6aef8b701",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9aeb455ecd134a29b93eb0651c5a9901",
            "value": 170498071
          }
        },
        "5c2e5d5f5b7e4e248c825d20e7f75c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20102e529da94e32bda8baaec05fcba0",
            "placeholder": "",
            "style": "IPY_MODEL_247119e71474422d8aac56e71a08ca34",
            "value": " 170499072/? [00:10&lt;00:00, 17118890.77it/s]"
          }
        },
        "8bb190912b114fa5858c4d5ec10a364c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc8d0ee7bf4487d930487ca2899d5d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11579493c8244feabdbcce11555b8687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26bca5a0e4b54cd4855bdaf6aef8b701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aeb455ecd134a29b93eb0651c5a9901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20102e529da94e32bda8baaec05fcba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "247119e71474422d8aac56e71a08ca34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}